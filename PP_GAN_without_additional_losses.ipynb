{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PP-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xTa6Dlcf4Gf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.io import read_image\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import csv\n",
        "import glob\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drcyaFPF0uOT"
      },
      "source": [
        "!gdown --id 1Of_EVz-yHV7QVWQGihYfvtny9Ne8qXVz -O CASIA-WebFace.zip\n",
        "!unzip /content/CASIA-WebFace.zip\n",
        "!rm /content/CASIA-WebFace.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2GVuxhsTYwW"
      },
      "source": [
        "row = []\n",
        "data = []\n",
        "\n",
        "labels_paths = glob.glob('/content/CASIA-WebFace/' + '*')\n",
        "n_labels = len(labels_paths)\n",
        "\n",
        "for i in range(n_labels):\n",
        "  for j in range(len(glob.glob(labels_paths[i] + '/*.jpg'))):\n",
        "    data.append([glob.glob(labels_paths[i] + '/*.jpg')[j], labels_paths[i].split('/')[-1]])\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTsS4iVEWNUa"
      },
      "source": [
        "with open('data.csv', 'w') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(['image', 'label'])\n",
        "  writer.writerows(data)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HlOhIvVzX6Z9",
        "outputId": "22f993dd-f7be-47f4-94fa-32b639910005"
      },
      "source": [
        "df = pd.read_csv('/content/data.csv')\n",
        "df = df.sample(frac=1)\n",
        "df.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>110519</th>\n",
              "      <td>/content/CASIA-WebFace/0355910/029.jpg</td>\n",
              "      <td>355910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346966</th>\n",
              "      <td>/content/CASIA-WebFace/1900981/034.jpg</td>\n",
              "      <td>1900981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328483</th>\n",
              "      <td>/content/CASIA-WebFace/1058940/041.jpg</td>\n",
              "      <td>1058940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196373</th>\n",
              "      <td>/content/CASIA-WebFace/1556320/123.jpg</td>\n",
              "      <td>1556320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247693</th>\n",
              "      <td>/content/CASIA-WebFace/0251986/071.jpg</td>\n",
              "      <td>251986</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         image    label\n",
              "110519  /content/CASIA-WebFace/0355910/029.jpg   355910\n",
              "346966  /content/CASIA-WebFace/1900981/034.jpg  1900981\n",
              "328483  /content/CASIA-WebFace/1058940/041.jpg  1058940\n",
              "196373  /content/CASIA-WebFace/1556320/123.jpg  1556320\n",
              "247693  /content/CASIA-WebFace/0251986/071.jpg   251986"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZw-WNku04Tk"
      },
      "source": [
        "class CasiaDataset(Dataset):\n",
        "  def __init__(self, \n",
        "               imgs_path,\n",
        "               csv_file,\n",
        "               transform\n",
        "               ):\n",
        "    self.imgs_path = imgs_path\n",
        "    self.data = pd.read_csv(csv_file)\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    self.data = self.data.sample(frac=1)\n",
        "    \n",
        "    image = read_image(self.data.iloc[index, 0])\n",
        "    label = self.data.iloc[index, 1]\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifJlENvU0338"
      },
      "source": [
        "transform = T.Compose([\n",
        "                       T.Resize(128),\n",
        "                       T.Grayscale()\n",
        "                       ])\n",
        "\n",
        "my_dataset = CasiaDataset('/content/CASIA-WebFace/',\n",
        "                          '/content/data.csv',\n",
        "                          transform)\n",
        "loader = DataLoader(my_dataset, \n",
        "                    batch_size=20, \n",
        "                    shuffle=False)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thYUeyxo03n5"
      },
      "source": [
        "images, labels = next(iter(loader))\n",
        "# print(images.shape)\n",
        "# print(len(labels))\n",
        "# print(labels)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAorGFFytVfK",
        "outputId": "4ec22dec-a462-43a9-d918-7f892954d48c"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB3Z4Tv-fe6a"
      },
      "source": [
        "class BlockDiscriminator(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch, stride, type):\n",
        "    super().__init__()\n",
        "    if type == 1:\n",
        "      self.block_disc = nn.Sequential(\n",
        "          nn.Conv2d(in_ch, out_ch, kernel_size=2)\n",
        "      )\n",
        "    elif type == 2:\n",
        "      self.block_disc = nn.Sequential(\n",
        "          nn.Conv2d(in_ch, out_ch, kernel_size=3, stride = stride, padding = 1),\n",
        "          nn.BatchNorm2d(out_ch),\n",
        "          nn.LeakyReLU()\n",
        "      )\n",
        "    else:\n",
        "      self.block_disc = nn.Sequential(\n",
        "          nn.Conv2d(in_ch, out_ch, kernel_size=3, stride = stride),\n",
        "          nn.LeakyReLU()\n",
        "      )\n",
        "  \n",
        "  def forward(self, input):\n",
        "    return self.block_disc(input)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTV3XG5Wfed4"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.disc = nn.ModuleList([\n",
        "                               BlockDiscriminator(1, 1, 2, type=1),\n",
        "                               BlockDiscriminator(1, 1, 2, type=2),\n",
        "                               BlockDiscriminator(1, 1, 2, type=2),\n",
        "                               BlockDiscriminator(1, 1, 1, type=2),\n",
        "                               BlockDiscriminator(1, 1, 1, type=3),\n",
        "                               nn.Sigmoid()\n",
        "    ])\n",
        "  \n",
        "  def forward(self, input):\n",
        "    for layer in self.disc:\n",
        "      output = layer(input)\n",
        "      input = output\n",
        "    return output\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqNmM0vSn39B",
        "outputId": "ee856c3e-79c1-4595-8e93-fc81f7374e67"
      },
      "source": [
        "input = torch.randn(1, 1, 128, 128)\n",
        "model = Discriminator()\n",
        "output = model(input)\n",
        "output.shape"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 30, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOWqOajhNr0o"
      },
      "source": [
        "## Encoder as U-Net\n",
        "class Enc_block(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch):\n",
        "    super().__init__()\n",
        "    self.enc_block = nn.Sequential(\n",
        "        nn.Conv2d(in_ch, out_ch, kernel_size = 4, stride = 2, padding = 1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.enc_block(input)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e9MOjrkGB0n"
      },
      "source": [
        "class Dec_block(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch):\n",
        "    super().__init__()\n",
        "    self.dec_block = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_ch, out_ch, kernel_size = 4, stride = 2, padding = 1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.dec_block(input)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpjUU2V-Nrkm"
      },
      "source": [
        "class Unet(nn.Module):\n",
        "  def __init__(self, in_ch = 1, out_ch = 1, features=[4, 8, 16, 32, 64, 128]):\n",
        "    super().__init__()\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.downs = nn.ModuleList([\n",
        "                                Enc_block(4, 8),\n",
        "                                Enc_block(8, 16),\n",
        "                                Enc_block(16, 32),\n",
        "                                Enc_block(32, 64),\n",
        "                                Enc_block(64, 128),\n",
        "                                nn.Sequential(\n",
        "                                    nn.Conv2d(128, 256, kernel_size = 4, stride = 2, padding = 1), \n",
        "                                    nn.LeakyReLU(),\n",
        "                                )                                \n",
        "    ])\n",
        "\n",
        "    self.ups = nn.ModuleList([\n",
        "                              nn.Sequential(\n",
        "                                  nn.ConvTranspose2d(512, 128, kernel_size=3),\n",
        "                                  nn.BatchNorm2d(128),\n",
        "                                  nn.Dropout(0.5),\n",
        "                                  nn.ReLU(inplace=True)),\n",
        "                              Dec_block(256, 64),\n",
        "                              Dec_block(128, 32),\n",
        "                              Dec_block(64, 16),\n",
        "                              Dec_block(32, 8),\n",
        "                              nn.Sequential(\n",
        "                                  nn.ConvTranspose2d(16, 4, kernel_size=4, stride=2, padding=1),\n",
        "                                  nn.BatchNorm2d(4),\n",
        "                              )\n",
        "    ])\n",
        "    \n",
        "    self.bottleneck = nn.Sequential(\n",
        "        nn.ConvTranspose2d(256, 256, kernel_size=4, padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "\n",
        "    self.final_conv = nn.Sequential(\n",
        "        nn.ConvTranspose2d(4, 1, kernel_size=5),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    skip_connections = []\n",
        "    inp_mod = nn.Conv2d(1, 4, kernel_size = 4)\n",
        "    input = inp_mod(input)\n",
        "    for down in self.downs:\n",
        "      input = down(input)\n",
        "      skip_connections.append(input)\n",
        "\n",
        "    input = self.bottleneck(input)\n",
        "    skip_connections = skip_connections[::-1] ### reversed list\n",
        "    \n",
        "    skip_connections_shape = []\n",
        "    for i in skip_connections:\n",
        "      skip_connections_shape.append(i.shape)\n",
        "\n",
        "    for idx in range(0, len(self.ups), 1):\n",
        "      skip_connection = skip_connections[idx]\n",
        "      if input.shape != skip_connection.shape:\n",
        "        input = TF.resize(input, size = skip_connection.shape[2:])\n",
        "      concat_skip = torch.cat((skip_connection, input), dim = 1)\n",
        "      input = self.ups[idx](concat_skip)\n",
        "      \n",
        "    return self.final_conv(input)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUjWp7kzoESp",
        "outputId": "20ff524b-5df1-4fd7-ce32-65a564905db8"
      },
      "source": [
        "input = torch.randn(1, 1, 128, 128)\n",
        "model = Unet()\n",
        "output = model(input)\n",
        "output.shape"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 128, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL7LH1__QJLc"
      },
      "source": [
        "discriminator = Discriminator().to(device)\n",
        "generator = Unet(n_noise).to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "D_opt = torch.optim.Adam(discriminator.parameters(), lr = 0.002)\n",
        "G_opt = torch.optim.Adam(generator.parameters(), lr = 0.002)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR-LyZe1NrJX"
      },
      "source": [
        "lr = 3e-4\n",
        "batch_size = 20\n",
        "num_epochs = 10\n",
        "n_noise = 512\n",
        "step = 0\n",
        "n_critic = 1\n",
        "\n",
        "# fixed_noise = torch.randn(batch_size, n_noise, 4, 4).to(device)\n",
        "\n",
        "D_labels = torch.ones(batch_size, 1, 30, 30).to(device)\n",
        "D_fakes = -torch.ones(batch_size, 1, 30, 30).to(device)\n"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMBwJ2lXNqv_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bbd8e2af-a610-4d67-f2a1-a2a1a8328169"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for idx, (images, _) in enumerate(loader):\n",
        "    x = images.float().to(device) # real\n",
        "    x_output = discriminator(x) # disc of real \n",
        "    D_x_loss = criterion(x_output, D_labels) # loss disc real \n",
        "\n",
        "    z = torch.randn(batch_size, 1, 128, 128).to(device) # noise\n",
        "    z_output = generator(z) # gen fake\n",
        "    z_output = discriminator(generator(z)) # disc of fake \n",
        "    D_z_loss = criterion(z_output, D_fakes) # loss disc fake\n",
        "    D_loss = D_x_loss + D_z_loss # total loss disc\n",
        "\n",
        "    D_opt.zero_grad()\n",
        "    D_loss.backward()\n",
        "    D_opt.step()\n",
        "\n",
        "    # Training generator\n",
        "    if step % n_critic == 0:\n",
        "      z = torch.randn(batch_size, 1, 128, 128).to(device)\n",
        "      z_outputs = discriminator(generator(z))\n",
        "      G_loss = criterion(z_outputs, D_labels)\n",
        "\n",
        "      G_opt.zero_grad()\n",
        "      G_loss.backward()\n",
        "      G_opt.step()\n",
        "        \n",
        "   \n",
        "  print(\n",
        "      f\"Epoch [{epoch} / {num_epochs}] \\ \"\n",
        "      f\"Loss D: {D_loss:.4f}, Loss G: {G_loss:.4f}\"\n",
        "  )"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-f201408dd35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# disc of real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mD_x_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss disc real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-7be2c91e4040>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGrayscaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \"\"\"\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mrgb_to_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional_tensor.py\u001b[0m in \u001b[0;36mrgb_to_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input image tensor should have at least 3 dimensions, but found {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0m_assert_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_output_channels\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional_tensor.py\u001b[0m in \u001b[0;36m_assert_channels\u001b[0;34m(img, permitted)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_num_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpermitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input image tensor permitted channel values are {}, but found {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermitted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Input image tensor permitted channel values are [3], but found 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRjAU_jrRDdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8hp8DCuqlDX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg_aARKoLrTY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B81LhN9ULrcS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}