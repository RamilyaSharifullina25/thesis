{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55498611",
   "metadata": {},
   "source": [
    "# Тестовое задание: Графовые нейронные сети, классические алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3065049",
   "metadata": {},
   "source": [
    "## Настройка окружения\n",
    "\n",
    "```\n",
    "!conda install -c dglteam dgl\n",
    "```\n",
    "\n",
    "_Примечание 1_. Для установки пакета с возможностью обучения моделей на видеокартах необходимо также иметь установленный CUDA.\n",
    "\n",
    "_Примечание 2_. dgl поддерживает работу с тремя фреймворками: PyTorch 1.5.0+, Apache MXNet 1.6+ и TensorFlow 2.3+. Мы работаем с PyTorch. Полный набор команд для развертывания окружения с возможностью обучения на GPU (cuda версии 10.2):\n",
    "\n",
    "```cmd\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "python -c \"import torch; print(torch.version.cuda)\"\n",
    "conda install -c dglteam dgl-cuda10.2\n",
    "python -m dgl.backend.set_default_backend pytorch\n",
    "python -c \"import dgl; print(dgl.__version__)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263c9b5",
   "metadata": {},
   "source": [
    "DGL (Deep Graph Library)  - это пакет, разработанный для обучения и использования моделей графовых нейронных сетей. В данном пакете реализовано большое количество ставших уже классическими модулей (например, GraphConv, GraphSage и т.д.), на основе которых можно разрабатывать собственные модели. DGL предоставляет большой набор возможностей: работа с GPU, возможности для работы с большими графами и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dbd626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dgl\n",
    "import dgl.nn as gnn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80cc6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# доп библиотеки\n",
    "from dgl.data import CoraGraphDataset\n",
    "import itertools\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f95ed",
   "metadata": {},
   "source": [
    "## Работа с графами в `dgl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ce9f5",
   "metadata": {},
   "source": [
    "В `dgl` граф - объект класс `DGLGraph`. Для его создания нужно указать:\n",
    "* тензор начальных узлов для ребер\n",
    "* тензор конечных узлов для ребер\n",
    "* кол-во вершин (узлы нумеруются, начиная с 0; кол-во вершин можно не задавать, если они все перечислены в списках для создания ребер)\n",
    "\n",
    "В `dgl` граф всегда ориентированный.\n",
    "\n",
    "На узлах (`.ndata`) и ребрах (`.edata`) могут храниться атрибуты. Их особенности:\n",
    "* только числовые тензоры;\n",
    "* атрибуты всех узлов (ребер) должны иметь одинаковый размер;\n",
    "* каждый атрибут имеет уникальное имя (атрибуты узлов и ребер могут иметь одинаковые имена);\n",
    "* первая размерность тензора атрибутов должна быть равна кол-ву узлов (ребер);\n",
    "* срез по строкам возвращает атрибуты одного узла (ребра)\n",
    "\n",
    "Пример создания графа:\n",
    "```python\n",
    "G = dgl.graph(([0, 0, 0, 0, 0], [1, 2, 3, 4, 5]), num_nodes=6)\n",
    "G.ndata['x'] = torch.randn(6, 3)\n",
    "G.ndata['y'] = torch.randint(0, 2, (6, ))\n",
    "```\n",
    "\n",
    "Кроме возможностей по созданию наборов данных, `dgl` предоставляет большой набор готовых датасетов в `dgl.data`.\n",
    "\n",
    "`DGLGraph` имеют большое количество свойств и методов для работы с графовыми структурами, которые можно найти в [документации](https://docs.dgl.ai/api/python/dgl.DGLGraph.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6474e5e3",
   "metadata": {},
   "source": [
    "### Задание 1. \n",
    "\n",
    "Загрузите датасет `CoraGraphDataset` из `dgl.data`. Этот датасет состоит из одного графа. Выведите на экран:\n",
    "\n",
    "* количество узлов в графе;\n",
    "* количество ребер в графе;\n",
    "* размерность атрибутов на узлах;\n",
    "* количество классов узлов в датасете\n",
    "\n",
    "Выделите подграф (`dgl.node_subgraph`), содержащий узлы, относящиеся к трем наиболее часто встречающимся классам. __1 балл__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d8bb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = CoraGraphDataset()\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2392022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label = {k:v for k, v in zip(g.ndata['label'].unique(),\n",
    "                                [(g.ndata['label'] == x_u).sum() for x_u in g.ndata['label'].unique()]\n",
    "                               )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c169d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor(0): tensor(351),\n",
       " tensor(1): tensor(217),\n",
       " tensor(2): tensor(418),\n",
       " tensor(3): tensor(818),\n",
       " tensor(4): tensor(426),\n",
       " tensor(5): tensor(298),\n",
       " tensor(6): tensor(180)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60d4327d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=3, num_edges=4,\n",
       "      ndata_schemes={'feat': Scheme(shape=(1433,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), '_ID': Scheme(shape=(), dtype=torch.int64)}\n",
       "      edata_schemes={'__orig__': Scheme(shape=(), dtype=torch.int64), '_ID': Scheme(shape=(), dtype=torch.int64)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.node_subgraph(g, [2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c78ca",
   "metadata": {},
   "source": [
    "### Задание 2. \n",
    "\n",
    "Файл `cities_nodes.csv` содержит данные о городах: название, страна, население, дату основания и координаты. В файле `cities_edges.csv` содержатся данные о связях между городам: города соединяются ребрами в случае, если в статье русскоязычной Википедии о городе присутсвует ссылка на страницу для другого города. Создайте на основе двух этих файлов `dgl.graph`. Для этого:\n",
    "\n",
    "\n",
    "1. занумеруйте города (узлы) целыми числами;\n",
    "\n",
    "2. представьте каждое ребро в виде пары двух целых чисел;\n",
    "\n",
    "3. в качестве атрибутов (`g.ndata['feat']`) узлов используйте логарифм численности населения в городе и входящую степень узлов (результат сохраните в виде двумерного тензора размера `(n_nodes, 2)` и приведите к `torch.float32`);\n",
    "\n",
    "4. занумеруйте страны целыми числами и в качестве меток узлов (`g.ndata['label']`) используйте номер соотвествующей страны; используйте тензор типа `torch.int64`;\n",
    "\n",
    "5. в качестве атрибутов (`g.edata['distance']`) ребер используйте логарифм расстояния между городами (результат сохраните в виде одномерного тензора размера `(n_edges, )` и приведите к `torch.float32`);\n",
    "\n",
    "6. добавьте графу атрибут `num_classes`, в котором хранится число классов (стран);\n",
    "\n",
    "7. разбейте множество узлов на обучающее и валидационное множество в соотношении 90%-10%. Для представления каждого из множеств создайте булев тензор $t$ размерности, равной количеству узлов в графе, где $t[i]==True$, если узел $i$ входит в соответствующее множество. Добавьте в словарь `ndata` два ключа `train_mask`, `val_mask`. __(1 балл)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406dfe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country_label</th>\n",
       "      <th>population</th>\n",
       "      <th>inception</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Минск</td>\n",
       "      <td>Белоруссия</td>\n",
       "      <td>2009786</td>\n",
       "      <td>01.01.1067</td>\n",
       "      <td>27.561837</td>\n",
       "      <td>53.902246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Гомель</td>\n",
       "      <td>Белоруссия</td>\n",
       "      <td>510300</td>\n",
       "      <td>01.01.1142</td>\n",
       "      <td>30.983333</td>\n",
       "      <td>52.441667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Брест</td>\n",
       "      <td>Белоруссия</td>\n",
       "      <td>340318</td>\n",
       "      <td>01.01.1017</td>\n",
       "      <td>23.656944</td>\n",
       "      <td>52.084722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Гродно</td>\n",
       "      <td>Белоруссия</td>\n",
       "      <td>356900</td>\n",
       "      <td>01.01.1128</td>\n",
       "      <td>23.816667</td>\n",
       "      <td>53.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Вилейка</td>\n",
       "      <td>Белоруссия</td>\n",
       "      <td>27167</td>\n",
       "      <td>25.11.1460</td>\n",
       "      <td>26.916667</td>\n",
       "      <td>54.483333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 country_label  population   inception        lat       long\n",
       "0      Минск    Белоруссия     2009786  01.01.1067  27.561837  53.902246\n",
       "1     Гомель    Белоруссия      510300  01.01.1142  30.983333  52.441667\n",
       "2      Брест    Белоруссия      340318  01.01.1017  23.656944  52.084722\n",
       "3     Гродно    Белоруссия      356900  01.01.1128  23.816667  53.666667\n",
       "4    Вилейка    Белоруссия       27167  25.11.1460  26.916667  54.483333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes = pd.read_csv('cities_nodes.csv')\n",
    "df_edges = pd.read_csv('cities_edges.csv')\n",
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea678d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим словарь {'название_города', номер} \n",
    "city_dir = {k:v for k, v in zip(df_nodes['Unnamed: 0'],\n",
    "                                [i for i in range(0, len(df_nodes['Unnamed: 0']))]\n",
    "                               )}\n",
    "# создадим граф\n",
    "g = dgl.graph((torch.from_numpy(df_edges.source.map(city_dir).values), \n",
    "               torch.from_numpy(df_edges.target.map(city_dir).values)), num_nodes=315)\n",
    "\n",
    "# присвоим атрибутам узлов feat нужные значения\n",
    "g.ndata['feat'] = torch.stack((torch.log(torch.from_numpy(df_nodes.population.values)), \\\n",
    "                               g.in_degrees()), dim = 1).type(torch.float32)\n",
    "\n",
    "\n",
    "# создадим словарь {'навзание_страны', номер}\n",
    "country_dir = {k:v for k, v in zip(df_nodes.country_label.unique(),\n",
    "                                [i for i in range(0, len(df_nodes.country_label.value_counts()))]\n",
    "                               )}\n",
    "\n",
    "# присвоим атрибутам узлов label нужные значения\n",
    "g.ndata['label'] = torch.from_numpy(df_nodes.country_label.map(country_dir).values).type(torch.int64)\n",
    "\n",
    "# присвоим атрибутам ребер distance нужные значения\n",
    "g.edata['distance'] = torch.log(torch.from_numpy(df_edges.distance.values)).type(torch.float32)\n",
    "\n",
    "# присвоим атрибутам узлов num_classes нужные значения\n",
    "g.ndata['num_classes'] = torch.ones(315) * 4\n",
    "\n",
    "# разобъем выборку на train и val\n",
    "n_nodes = df_nodes.shape[0]\n",
    "n_train = int(n_nodes * 0.9)\n",
    "n_val = int(n_nodes * 0.1)\n",
    "train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "train_mask[:n_train] = True\n",
    "val_mask[n_train:] = True\n",
    "\n",
    "# присвоим атрибутам узлов train_mask и val_mask нужные значения\n",
    "g.ndata['train_mask'] = train_mask\n",
    "g.ndata['val_mask'] = val_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00636b",
   "metadata": {},
   "source": [
    "## Построение нейронных сетей с использованием `dgl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ca4681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12666])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edata['distance'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c851233",
   "metadata": {},
   "source": [
    "`dgl` предоставляет [большое количество](https://docs.dgl.ai/api/python/nn.html) модулей-блоков для построения нейронных сетей. С точки зрения реализации они являются наследниками стандартного `torch.nn.Module`. Поэтому общая логика написания сети остается точно такой же, как и при использовании `torch`: вы описываете модель при помощи класса-наследника `torch.nn.Module`, определяете в методе `__init__` все необходимые блоки и реализуете метод `forward`, где вызываете модули в нужной последовательности. Кроме модулей графовых сетей вы можете использовать и \"классические\": `Linear`, `Dropout` и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d1932",
   "metadata": {},
   "source": [
    "В отличие от модулей из `torch`, где метод `forward` обычно ожидает один параметр на входе (как `Linear` ожидает один тензор), модули из `dgl` обычно ожидают 2 параметра: граф и матрицу атрибутов узлов. Конкретный модуль может ожидать и чего-то другого, поэтому рекомендуется сверяться с документацией."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3315372",
   "metadata": {},
   "source": [
    "Пример работы с модулями из `dgl`\n",
    "\n",
    "```python\n",
    "import dgl.nn as gnn\n",
    "g = dgl.graph(([0, 0, 0, 0, 0], [1, 2, 3, 4, 5]), num_nodes=6)\n",
    "g = dgl.add_reverse_edges(g)\n",
    "g.ndata['x'] = torch.randn(6, 3)\n",
    "\n",
    "conv = gnn.GraphConv(3, 32)\n",
    "h = conv(g, g.ndata['x'])\n",
    "h.shape # h - тензор скрытых представлений узлов после свертки conv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36434e5",
   "metadata": {},
   "source": [
    "Пример описания графовой нейронной сети при помощи `dgl`\n",
    "```python\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f172730",
   "metadata": {},
   "source": [
    "### Задание 3. \n",
    "Используя граф, полученный в задании 2, и графовую нейронную сеть, написанную с использованием библиотеки `dgl`, решите задачу классификации городов по странам (задача классификации узлов). __(2 балла)__\n",
    "\n",
    "На каждой эпохи обучения выводите на экран следующую информацию:\n",
    "1. Номер эпохи\n",
    "2. Значение функции потерь на обучающем множестве\n",
    "3. Значение метрики accuracy (`sklearn.metrics.accuracy_score`) на обучающем множестве\n",
    "3. Значение метрики accuracy (`sklearn.metrics.accuracy_score`) на тестовом множестве\n",
    "\n",
    "Важно: при расчете значения функции потерь при обучении используйте только узлы, относящиеся к обучающему множеству! Узлы, относящиеся к валидационному множеству, для простоты можно оставить в графе, но прогнозы для этих узлов не должны оказывать влияние на функцию потерь и обратное распространение ошибки при обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ae9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d024748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aed5c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(g.ndata['feat'].shape[1], 16, len(df_nodes.country_label.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79e3b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 4.377, train acc: 0.251, val acc: 0.000\n",
      "In epoch 1, loss: 3.427, train acc: 0.226, val acc: 0.000\n",
      "In epoch 2, loss: 2.806, train acc: 0.226, val acc: 0.000\n",
      "In epoch 3, loss: 2.270, train acc: 0.230, val acc: 0.000\n",
      "In epoch 4, loss: 2.214, train acc: 0.276, val acc: 0.000\n",
      "In epoch 5, loss: 2.284, train acc: 0.279, val acc: 0.062\n",
      "In epoch 6, loss: 2.118, train acc: 0.279, val acc: 0.094\n",
      "In epoch 7, loss: 1.822, train acc: 0.283, val acc: 0.312\n",
      "In epoch 8, loss: 1.576, train acc: 0.297, val acc: 0.406\n",
      "In epoch 9, loss: 1.503, train acc: 0.071, val acc: 0.875\n",
      "In epoch 10, loss: 1.515, train acc: 0.085, val acc: 0.969\n",
      "In epoch 11, loss: 1.502, train acc: 0.307, val acc: 0.969\n",
      "In epoch 12, loss: 1.435, train acc: 0.307, val acc: 0.969\n",
      "In epoch 13, loss: 1.320, train acc: 0.311, val acc: 0.969\n",
      "In epoch 14, loss: 1.180, train acc: 0.314, val acc: 0.969\n",
      "In epoch 15, loss: 1.062, train acc: 0.318, val acc: 0.969\n",
      "In epoch 16, loss: 1.009, train acc: 0.364, val acc: 0.969\n",
      "In epoch 17, loss: 1.008, train acc: 0.364, val acc: 0.969\n",
      "In epoch 18, loss: 0.994, train acc: 0.364, val acc: 0.969\n",
      "In epoch 19, loss: 0.935, train acc: 0.403, val acc: 0.969\n",
      "In epoch 20, loss: 0.846, train acc: 0.576, val acc: 0.969\n",
      "In epoch 21, loss: 0.762, train acc: 0.763, val acc: 0.969\n",
      "In epoch 22, loss: 0.717, train acc: 0.756, val acc: 0.969\n",
      "In epoch 23, loss: 0.703, train acc: 0.717, val acc: 0.969\n",
      "In epoch 24, loss: 0.690, train acc: 0.717, val acc: 0.969\n",
      "In epoch 25, loss: 0.655, train acc: 0.714, val acc: 0.969\n",
      "In epoch 26, loss: 0.601, train acc: 0.714, val acc: 0.969\n",
      "In epoch 27, loss: 0.549, train acc: 0.753, val acc: 0.969\n",
      "In epoch 28, loss: 0.517, train acc: 0.760, val acc: 0.969\n",
      "In epoch 29, loss: 0.504, train acc: 0.760, val acc: 0.969\n",
      "In epoch 30, loss: 0.495, train acc: 0.763, val acc: 0.969\n",
      "In epoch 31, loss: 0.471, train acc: 0.763, val acc: 0.969\n",
      "In epoch 32, loss: 0.450, train acc: 0.760, val acc: 0.969\n",
      "In epoch 33, loss: 0.438, train acc: 0.763, val acc: 0.969\n",
      "In epoch 34, loss: 0.437, train acc: 0.975, val acc: 0.969\n",
      "In epoch 35, loss: 0.437, train acc: 0.717, val acc: 0.969\n",
      "In epoch 36, loss: 0.428, train acc: 0.922, val acc: 0.969\n",
      "In epoch 37, loss: 0.415, train acc: 0.756, val acc: 0.969\n",
      "In epoch 38, loss: 0.407, train acc: 0.760, val acc: 0.969\n",
      "In epoch 39, loss: 0.407, train acc: 0.763, val acc: 0.969\n",
      "In epoch 40, loss: 0.407, train acc: 0.760, val acc: 0.969\n",
      "In epoch 41, loss: 0.403, train acc: 0.760, val acc: 0.969\n",
      "In epoch 42, loss: 0.396, train acc: 0.763, val acc: 0.969\n",
      "In epoch 43, loss: 0.391, train acc: 0.756, val acc: 0.969\n",
      "In epoch 44, loss: 0.390, train acc: 0.767, val acc: 0.969\n",
      "In epoch 45, loss: 0.390, train acc: 0.975, val acc: 0.969\n",
      "In epoch 46, loss: 0.386, train acc: 0.975, val acc: 0.969\n",
      "In epoch 47, loss: 0.381, train acc: 0.760, val acc: 0.969\n",
      "In epoch 48, loss: 0.377, train acc: 0.756, val acc: 0.969\n",
      "In epoch 49, loss: 0.376, train acc: 0.763, val acc: 0.969\n",
      "In epoch 50, loss: 0.374, train acc: 0.763, val acc: 0.969\n",
      "In epoch 51, loss: 0.371, train acc: 0.760, val acc: 0.969\n",
      "In epoch 52, loss: 0.367, train acc: 0.756, val acc: 0.969\n",
      "In epoch 53, loss: 0.365, train acc: 0.763, val acc: 0.969\n",
      "In epoch 54, loss: 0.364, train acc: 0.975, val acc: 0.969\n",
      "In epoch 55, loss: 0.362, train acc: 0.975, val acc: 0.969\n",
      "In epoch 56, loss: 0.358, train acc: 0.767, val acc: 0.969\n",
      "In epoch 57, loss: 0.356, train acc: 0.753, val acc: 0.969\n",
      "In epoch 58, loss: 0.355, train acc: 0.760, val acc: 0.969\n",
      "In epoch 59, loss: 0.353, train acc: 0.756, val acc: 0.969\n",
      "In epoch 60, loss: 0.351, train acc: 0.753, val acc: 0.969\n",
      "In epoch 61, loss: 0.348, train acc: 0.767, val acc: 0.969\n",
      "In epoch 62, loss: 0.347, train acc: 0.975, val acc: 0.969\n",
      "In epoch 63, loss: 0.345, train acc: 0.975, val acc: 0.969\n",
      "In epoch 64, loss: 0.343, train acc: 0.795, val acc: 0.969\n",
      "In epoch 65, loss: 0.341, train acc: 0.760, val acc: 0.969\n",
      "In epoch 66, loss: 0.339, train acc: 0.760, val acc: 0.969\n",
      "In epoch 67, loss: 0.337, train acc: 0.760, val acc: 0.969\n",
      "In epoch 68, loss: 0.335, train acc: 0.795, val acc: 0.969\n",
      "In epoch 69, loss: 0.333, train acc: 0.975, val acc: 0.969\n",
      "In epoch 70, loss: 0.331, train acc: 0.975, val acc: 0.969\n",
      "In epoch 71, loss: 0.329, train acc: 0.975, val acc: 0.969\n",
      "In epoch 72, loss: 0.327, train acc: 0.795, val acc: 0.969\n",
      "In epoch 73, loss: 0.326, train acc: 0.784, val acc: 0.969\n",
      "In epoch 74, loss: 0.324, train acc: 0.795, val acc: 0.969\n",
      "In epoch 75, loss: 0.322, train acc: 0.975, val acc: 0.969\n",
      "In epoch 76, loss: 0.320, train acc: 0.975, val acc: 0.969\n",
      "In epoch 77, loss: 0.318, train acc: 0.975, val acc: 0.969\n",
      "In epoch 78, loss: 0.316, train acc: 0.975, val acc: 0.969\n",
      "In epoch 79, loss: 0.315, train acc: 0.802, val acc: 0.969\n",
      "In epoch 80, loss: 0.313, train acc: 0.975, val acc: 0.969\n",
      "In epoch 81, loss: 0.311, train acc: 0.975, val acc: 0.969\n",
      "In epoch 82, loss: 0.309, train acc: 0.975, val acc: 0.969\n",
      "In epoch 83, loss: 0.307, train acc: 0.975, val acc: 0.969\n",
      "In epoch 84, loss: 0.305, train acc: 0.975, val acc: 0.969\n",
      "In epoch 85, loss: 0.303, train acc: 0.975, val acc: 0.969\n",
      "In epoch 86, loss: 0.301, train acc: 0.975, val acc: 0.969\n",
      "In epoch 87, loss: 0.300, train acc: 0.975, val acc: 0.969\n",
      "In epoch 88, loss: 0.298, train acc: 0.975, val acc: 0.969\n",
      "In epoch 89, loss: 0.296, train acc: 0.975, val acc: 0.969\n",
      "In epoch 90, loss: 0.294, train acc: 0.975, val acc: 0.969\n",
      "In epoch 91, loss: 0.292, train acc: 0.975, val acc: 0.969\n",
      "In epoch 92, loss: 0.290, train acc: 0.975, val acc: 0.969\n",
      "In epoch 93, loss: 0.289, train acc: 0.975, val acc: 0.969\n",
      "In epoch 94, loss: 0.287, train acc: 0.975, val acc: 0.969\n",
      "In epoch 95, loss: 0.285, train acc: 0.975, val acc: 0.969\n",
      "In epoch 96, loss: 0.283, train acc: 0.975, val acc: 0.969\n",
      "In epoch 97, loss: 0.281, train acc: 0.975, val acc: 0.969\n",
      "In epoch 98, loss: 0.279, train acc: 0.975, val acc: 0.969\n",
      "In epoch 99, loss: 0.278, train acc: 0.975, val acc: 0.969\n"
     ]
    }
   ],
   "source": [
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    for e in range(100):\n",
    "        logits = model(g, features)\n",
    "\n",
    "        pred = logits.argmax(1)\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "#         train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        train_acc = accuracy_score(pred[train_mask], labels[train_mask])\n",
    "#         val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        val_acc = accuracy_score(pred[val_mask], labels[val_mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('In epoch {}, loss: {:.3f}, train acc: {:.3f}, val acc: {:.3f}'.format(\n",
    "            e, loss, train_acc, val_acc))\n",
    "\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265744f",
   "metadata": {},
   "source": [
    "### Задание 4. \n",
    "Используя граф, полученный в задании 2, и графовую нейронную сеть, написанную с использованием библиотеки `dgl`, решите задачу предсказания связей. __(2 балла)__\n",
    "\n",
    "Цикл обучения должен состоять из следующих шагов:\n",
    "1. получение представлений `h` узлов при помощи сети, аналогичной заданию 3;\n",
    "2. конструирование графа отрицательных примеров (см функцию `construct_negative_graph`);\n",
    "3. расчет прогнозов ребер на основе исходного графа и скрытых представлений узлов `h` (см класс `MLPPredictor`) - для этих ребер модель должна предсказывать метку 1;\n",
    "4. расчет прогнозов ребер на основе графа отрицательных примеров и и скрытых представлений узлов `h` (см класс `MLPPredictor`) - для этих ребер модель должна предсказывать метку 0;\n",
    "5. расчет функции потерь (например, `CrossEntropyLoss`) и шаг оптимизации.\n",
    "\n",
    "На каждой эпохи обучения выводите на экран следующую информацию:\n",
    "1. Номер эпохи;\n",
    "2. Значение функции потерь на обучающем множестве.\n",
    "\n",
    "Примечания: \n",
    "1. для простоты в данной задаче не предполагается разбиение ребер на обучающее и тестовое множество;\n",
    "2. на каждой эпохе получение эмбеддингов `h` происходит только 1 раз на основе исходного графа;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5af13b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for training and validation set\n",
    "\n",
    "u, v = g.edges()\n",
    "\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "\n",
    "train_pos_u, train_pos_v = u[eids], v[eids]\n",
    "\n",
    "def construct_negative_graph(g, k):\n",
    "    '''Берет ребра из графа g и случайным образом заменяет вершины на концах ребер'''\n",
    "    u, v = g.edges()\n",
    "    neg_u = u.repeat_interleave(k).long()\n",
    "    neg_v = torch.randint(0, g.num_nodes(), (len(neg_u),)).long()\n",
    "    return dgl.graph((neg_u, neg_v), num_nodes=g.num_nodes())\n",
    "\n",
    "neg_u, neg_v = construct_negative_graph(g, 1).edges()\n",
    "neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids], neg_v[neg_eids]\n",
    "\n",
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13dc0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# создадим модель\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    '''Берет представления узлов, находящихся на концах ребер, конкатенирует их и прогоняет через небольшую полносвязную\n",
    "    сеть для получения прогноза вероятности существования ребра'''\n",
    "    def __init__(self, h_feats, out_feats=1):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, out_feats)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd4e5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = dgl.remove_edges(g, eids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3be38504",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphSAGE(train_g.ndata['feat'].shape[1], 16)\n",
    "pred = MLPPredictor(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0172bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 6.4814372062683105\n",
      "In epoch 1, loss: 1.5469502210617065\n",
      "In epoch 2, loss: 4.237903594970703\n",
      "In epoch 3, loss: 4.325287818908691\n",
      "In epoch 4, loss: 2.874101400375366\n",
      "In epoch 5, loss: 1.0443806648254395\n",
      "In epoch 6, loss: 1.0991381406784058\n",
      "In epoch 7, loss: 1.8753777742385864\n",
      "In epoch 8, loss: 2.0923545360565186\n",
      "In epoch 9, loss: 1.9056904315948486\n",
      "In epoch 10, loss: 1.487423062324524\n",
      "In epoch 11, loss: 0.997351348400116\n",
      "In epoch 12, loss: 0.6487745642662048\n",
      "In epoch 13, loss: 0.6909669041633606\n",
      "In epoch 14, loss: 0.9358013272285461\n",
      "In epoch 15, loss: 1.0631556510925293\n",
      "In epoch 16, loss: 1.0176939964294434\n",
      "In epoch 17, loss: 0.8556585907936096\n",
      "In epoch 18, loss: 0.6759239435195923\n",
      "In epoch 19, loss: 0.5824633240699768\n",
      "In epoch 20, loss: 0.587801456451416\n",
      "In epoch 21, loss: 0.6367713212966919\n",
      "In epoch 22, loss: 0.6764504909515381\n",
      "In epoch 23, loss: 0.6747308373451233\n",
      "In epoch 24, loss: 0.6343117952346802\n",
      "In epoch 25, loss: 0.6289168000221252\n",
      "In epoch 26, loss: 0.668972909450531\n",
      "In epoch 27, loss: 0.6485721468925476\n",
      "In epoch 28, loss: 0.6125794053077698\n",
      "In epoch 29, loss: 0.6044714450836182\n",
      "In epoch 30, loss: 0.6110411882400513\n",
      "In epoch 31, loss: 0.6066359281539917\n",
      "In epoch 32, loss: 0.585725724697113\n",
      "In epoch 33, loss: 0.5641863346099854\n",
      "In epoch 34, loss: 0.5500192642211914\n",
      "In epoch 35, loss: 0.5423968434333801\n",
      "In epoch 36, loss: 0.5479040145874023\n",
      "In epoch 37, loss: 0.5558376312255859\n",
      "In epoch 38, loss: 0.5552138686180115\n",
      "In epoch 39, loss: 0.548770010471344\n",
      "In epoch 40, loss: 0.5424612164497375\n",
      "In epoch 41, loss: 0.5298200249671936\n",
      "In epoch 42, loss: 0.5226888656616211\n",
      "In epoch 43, loss: 0.5171250700950623\n",
      "In epoch 44, loss: 0.5138932466506958\n",
      "In epoch 45, loss: 0.513468325138092\n",
      "In epoch 46, loss: 0.5126044750213623\n",
      "In epoch 47, loss: 0.5107138156890869\n",
      "In epoch 48, loss: 0.5076968669891357\n",
      "In epoch 49, loss: 0.5036530494689941\n",
      "In epoch 50, loss: 0.4990672171115875\n",
      "In epoch 51, loss: 0.4947974383831024\n",
      "In epoch 52, loss: 0.49170389771461487\n",
      "In epoch 53, loss: 0.49022528529167175\n",
      "In epoch 54, loss: 0.4898286759853363\n",
      "In epoch 55, loss: 0.489685982465744\n",
      "In epoch 56, loss: 0.4889988601207733\n",
      "In epoch 57, loss: 0.48730039596557617\n",
      "In epoch 58, loss: 0.48485851287841797\n",
      "In epoch 59, loss: 0.48219385743141174\n",
      "In epoch 60, loss: 0.4798656404018402\n",
      "In epoch 61, loss: 0.47818905115127563\n",
      "In epoch 62, loss: 0.4770481586456299\n",
      "In epoch 63, loss: 0.4761751890182495\n",
      "In epoch 64, loss: 0.4751281142234802\n",
      "In epoch 65, loss: 0.47354745864868164\n",
      "In epoch 66, loss: 0.47153156995773315\n",
      "In epoch 67, loss: 0.4693238139152527\n",
      "In epoch 68, loss: 0.46724680066108704\n",
      "In epoch 69, loss: 0.4660646319389343\n",
      "In epoch 70, loss: 0.4656180441379547\n",
      "In epoch 71, loss: 0.4654306471347809\n",
      "In epoch 72, loss: 0.46509069204330444\n",
      "In epoch 73, loss: 0.46434295177459717\n",
      "In epoch 74, loss: 0.4632290303707123\n",
      "In epoch 75, loss: 0.4619865119457245\n",
      "In epoch 76, loss: 0.4608756899833679\n",
      "In epoch 77, loss: 0.46003031730651855\n",
      "In epoch 78, loss: 0.4593801200389862\n",
      "In epoch 79, loss: 0.4587398171424866\n",
      "In epoch 80, loss: 0.45799511671066284\n",
      "In epoch 81, loss: 0.45725926756858826\n",
      "In epoch 82, loss: 0.4565480649471283\n",
      "In epoch 83, loss: 0.45590606331825256\n",
      "In epoch 84, loss: 0.4553569257259369\n",
      "In epoch 85, loss: 0.45489075779914856\n",
      "In epoch 86, loss: 0.45440182089805603\n",
      "In epoch 87, loss: 0.4538657069206238\n",
      "In epoch 88, loss: 0.45328402519226074\n",
      "In epoch 89, loss: 0.4526979625225067\n",
      "In epoch 90, loss: 0.4521395266056061\n",
      "In epoch 91, loss: 0.45166081190109253\n",
      "In epoch 92, loss: 0.45125752687454224\n",
      "In epoch 93, loss: 0.4508901536464691\n",
      "In epoch 94, loss: 0.4504750370979309\n",
      "In epoch 95, loss: 0.44996076822280884\n",
      "In epoch 96, loss: 0.44937989115715027\n",
      "In epoch 97, loss: 0.44882503151893616\n",
      "In epoch 98, loss: 0.44838112592697144\n",
      "In epoch 99, loss: 0.44802114367485046\n"
     ]
    }
   ],
   "source": [
    "# зададим оптимизатор\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)\n",
    "\n",
    "# обучим модель \n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    h = model(train_g, train_g.ndata['feat'])\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('In epoch {}, loss: {}'.format(e, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "882659e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GC(g.ndata['feat'].shape[1], 16, len(df_nodes.country_label.value_counts()))\n",
    "# train_classifier(g, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa3684",
   "metadata": {},
   "source": [
    "### Задание 5. \n",
    "\n",
    "Используя граф, полученный в задании 2, и графовую нейронную сеть, написанную с использованием библиотеки `dgl`, решите задачу предсказания расстояния между городами (задача регрессии на ребрах). __(2 балла)__\n",
    "\n",
    "Цикл обучения должен состоять из следующих шагов:\n",
    "1. получение представлений `h` узлов при помощи сети, аналогичной заданию 3;\n",
    "3. расчет прогнозов ребер на основе исходного графа и скрытых представлений узлов `h` (см класс `MLPPredictor`);\n",
    "5. расчет функции потерь (например, `MSELoss`) и шаг оптимизации.\n",
    "\n",
    "На каждой эпохи обучения выводите на экран следующую информацию:\n",
    "1. Номер эпохи\n",
    "2. Значение функции потерь на обучающем множестве.\n",
    "\n",
    "Примечание: для простоты в данной задаче не предполагается разбиение ребер на обучающее и тестовое множество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b14d25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.sage = GCN(in_features, hidden_features, out_features)\n",
    "        self.pred = MLPPredictor(5)\n",
    "    def forward(self, g, x):\n",
    "        h = self.sage(g, x)\n",
    "        return self.pred(g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beee0218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: inf\n",
      "In epoch 1, loss: nan\n",
      "In epoch 2, loss: nan\n",
      "In epoch 3, loss: nan\n",
      "In epoch 4, loss: nan\n",
      "In epoch 5, loss: nan\n",
      "In epoch 6, loss: nan\n",
      "In epoch 7, loss: nan\n",
      "In epoch 8, loss: nan\n",
      "In epoch 9, loss: nan\n",
      "In epoch 10, loss: nan\n",
      "In epoch 11, loss: nan\n",
      "In epoch 12, loss: nan\n",
      "In epoch 13, loss: nan\n",
      "In epoch 14, loss: nan\n",
      "In epoch 15, loss: nan\n",
      "In epoch 16, loss: nan\n",
      "In epoch 17, loss: nan\n",
      "In epoch 18, loss: nan\n",
      "In epoch 19, loss: nan\n",
      "In epoch 20, loss: nan\n",
      "In epoch 21, loss: nan\n",
      "In epoch 22, loss: nan\n",
      "In epoch 23, loss: nan\n",
      "In epoch 24, loss: nan\n",
      "In epoch 25, loss: nan\n",
      "In epoch 26, loss: nan\n",
      "In epoch 27, loss: nan\n",
      "In epoch 28, loss: nan\n",
      "In epoch 29, loss: nan\n",
      "In epoch 30, loss: nan\n",
      "In epoch 31, loss: nan\n",
      "In epoch 32, loss: nan\n",
      "In epoch 33, loss: nan\n",
      "In epoch 34, loss: nan\n",
      "In epoch 35, loss: nan\n",
      "In epoch 36, loss: nan\n",
      "In epoch 37, loss: nan\n",
      "In epoch 38, loss: nan\n",
      "In epoch 39, loss: nan\n",
      "In epoch 40, loss: nan\n",
      "In epoch 41, loss: nan\n",
      "In epoch 42, loss: nan\n",
      "In epoch 43, loss: nan\n",
      "In epoch 44, loss: nan\n",
      "In epoch 45, loss: nan\n",
      "In epoch 46, loss: nan\n",
      "In epoch 47, loss: nan\n",
      "In epoch 48, loss: nan\n",
      "In epoch 49, loss: nan\n",
      "In epoch 50, loss: nan\n",
      "In epoch 51, loss: nan\n",
      "In epoch 52, loss: nan\n",
      "In epoch 53, loss: nan\n",
      "In epoch 54, loss: nan\n",
      "In epoch 55, loss: nan\n",
      "In epoch 56, loss: nan\n",
      "In epoch 57, loss: nan\n",
      "In epoch 58, loss: nan\n",
      "In epoch 59, loss: nan\n",
      "In epoch 60, loss: nan\n",
      "In epoch 61, loss: nan\n",
      "In epoch 62, loss: nan\n",
      "In epoch 63, loss: nan\n",
      "In epoch 64, loss: nan\n",
      "In epoch 65, loss: nan\n",
      "In epoch 66, loss: nan\n",
      "In epoch 67, loss: nan\n",
      "In epoch 68, loss: nan\n",
      "In epoch 69, loss: nan\n",
      "In epoch 70, loss: nan\n",
      "In epoch 71, loss: nan\n",
      "In epoch 72, loss: nan\n",
      "In epoch 73, loss: nan\n",
      "In epoch 74, loss: nan\n",
      "In epoch 75, loss: nan\n",
      "In epoch 76, loss: nan\n",
      "In epoch 77, loss: nan\n",
      "In epoch 78, loss: nan\n",
      "In epoch 79, loss: nan\n",
      "In epoch 80, loss: nan\n",
      "In epoch 81, loss: nan\n",
      "In epoch 82, loss: nan\n",
      "In epoch 83, loss: nan\n",
      "In epoch 84, loss: nan\n",
      "In epoch 85, loss: nan\n",
      "In epoch 86, loss: nan\n",
      "In epoch 87, loss: nan\n",
      "In epoch 88, loss: nan\n",
      "In epoch 89, loss: nan\n",
      "In epoch 90, loss: nan\n",
      "In epoch 91, loss: nan\n",
      "In epoch 92, loss: nan\n",
      "In epoch 93, loss: nan\n",
      "In epoch 94, loss: nan\n",
      "In epoch 95, loss: nan\n",
      "In epoch 96, loss: nan\n",
      "In epoch 97, loss: nan\n",
      "In epoch 98, loss: nan\n",
      "In epoch 99, loss: nan\n"
     ]
    }
   ],
   "source": [
    "node_features = g.ndata['feat']\n",
    "edge_label = g.edata['distance']\n",
    "\n",
    "model = Model(2, 20, 5)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(100):\n",
    "    pred = model(g, node_features)\n",
    "    loss = ((pred - edge_label) ** 2).mean()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print('In epoch {}, loss: {}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7df9e3",
   "metadata": {},
   "source": [
    "### Задание 6. \n",
    "Сведите эмбеддинги узлов, полученные при обучении модели в задаче 5, к векторам размерности 2, используя метод главных компонент (`sklearn.decomposition.PCA`). Визуализируйте сеть, используя теперь для координат точек векторное представление узлов размерности 2. Покажите цветом узла страну, к которой относится город. __(1 балл)__ \n",
    "\n",
    "Для преобразования `DGLGraph` к `nx.Graph` можно воспользоваться функцией `dgl.to_networkx`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
