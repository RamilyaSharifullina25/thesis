{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h8-TMy69K89W"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import tqdm\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "\n",
    "sys.path.insert(0, '/home/sharifullina/thesis/Soft-DTW-Loss/')\n",
    "from sdtw_cuda_loss import SoftDTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3n2eJPXZDJZX",
    "outputId": "110bea7b-c981-4496-c4a9-672362f1e166"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_folder = './extracted/mwd/nopims/'\n",
    "target_folder = '/home/sharifullina/thesis/datasets/extracted/mwd/volve/'\n",
    "# target_folder = '/Users/ramilasarifullina/Desktop/extracted/well_logs/volve/'\n",
    "# target_folder = '/Users/ramilasarifullina/Desktop/extracted/well_logs/nopims/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf2PmKCgQgnU"
   },
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wells = os.listdir(target_folder)\n",
    "len(wells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_params_plus = ['DEPTH', 'ROPA', 'HKLA', 'WOB', 'SPPA', 'WELL']\n",
    "required_params = ['DEPTH', 'ROPA', 'HKLA', 'WOB', 'SPPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = [json.load(open(os.path.join(target_folder, wells[i]), 'r')) for i in range(len(wells))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all = [pd.DataFrame(logs[i]['data'], columns=[x['name'] for x in logs[i]['curves']]) for i in range(len(logs))]\n",
    "for i in range(len(dataset_all)):\n",
    "    dataset_all[i]['WELL'] = wells[i]\n",
    "\n",
    "dataset = []\n",
    "for df in dataset_all:\n",
    "    dataset.append(df[required_params_plus])\n",
    "    \n",
    "X_all = dataset[0]\n",
    "for i in range(1, len(dataset)):\n",
    "    X_all = pd.concat([X_all, dataset[i]], ignore_index = True)\n",
    "X_all = X_all.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "for i in range(0, len(X_all) - 64, 10):\n",
    "    X_list.append(X_all[i : 64 + i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list_minus = []\n",
    "for x in X_list:\n",
    "    X_list_minus.append(x.loc[:, x.columns != 'WELL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in X_list_minus:\n",
    "    X.append(i.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5920, 64, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.dstack(X)\n",
    "X = np.rollaxis(X, -1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_well_to_label = {w:i for i, w in enumerate(wells)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5920,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([map_well_to_label[df['WELL'].iloc[0]] for df in X_list])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, data, labels, feature_scales={}, N=25000, length_lim=90, sparse_rate=1):\n",
    "\n",
    "#         if len(data) > N:\n",
    "#             sub_inds = np.random.choice(len(data), N, replace=False)\n",
    "#         else:\n",
    "#             sub_inds = np.arange(len(data))\n",
    "        \n",
    "        self.data = data\n",
    "        self.data = torch.tensor(self.data).to(torch.float32)\n",
    "        self.labels = torch.tensor(labels)\n",
    "        self.sparse_rate = sparse_rate\n",
    "        self.feature_scales = feature_scales\n",
    "        \n",
    "        for f in range(data.shape[2]):\n",
    "            self.data[:, :, f] = (self.data[:, :, f].T - self.data[:, :, f].mean(axis=1)).T \n",
    "            \n",
    "        for f in range(data.shape[2]):\n",
    "            if f not in feature_scales:\n",
    "                std = self.data[:, :, f].ravel().std()\n",
    "                feature_scales[f] = std\n",
    "            self.data[:, :, f] = self.data[:, :, f]/feature_scales[f]\n",
    "            \n",
    "        self.data = self.data.transpose(1, 2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i], self.labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataSet at 0x7f316eaf9a30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dataset = DataSet(X, y, sparse_rate=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = 4000\n",
    "train_set, val_set = torch.utils.data.random_split(torch_dataset, [train_n, len(torch_dataset) - train_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 5, 64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for x_t, y_t in train_loader:\n",
    "    print(x_t.shape)\n",
    "    print(y_t.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Z464ltkLXG-r"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, 2, 2, 1, bias=False, padding_mode=\"reflect\")\n",
    "            if down\n",
    "            else nn.ConvTranspose1d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.down = down\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.dropout(x) if self.use_dropout else x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=len(required_params), features=32):\n",
    "        super().__init__()\n",
    "        self.initial_down = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down2 = Block(\n",
    "            features * 2, features * 4, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down3 = Block(\n",
    "            features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down4 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down5 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down6 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv1d(features * 8, features * 8, 4, 2, 1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
    "        self.up2 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
    "        )\n",
    "        self.up3 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
    "        )\n",
    "        self.up4 = Block(\n",
    "            features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up5 = Block(\n",
    "            features * 8, features * 2, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up6 = Block(\n",
    "            features * 4 , features, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.final_up = nn.Sequential(\n",
    "            nn.ConvTranspose1d(features * 2, in_channels, kernel_size=4, stride=2, padding=1),\n",
    "            # nn.Tanh(),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        d1 = self.initial_down(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d3 = self.down2(d2)\n",
    "        d4 = self.down3(d3)\n",
    "        d5 = self.down4(d4)\n",
    "        d6 = self.down5(d5)\n",
    "        return d6\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print('x', x.shape)\n",
    "        d1 = self.initial_down(x)\n",
    "#         print('d1', d1.shape)\n",
    "        d2 = self.down1(d1)\n",
    "#         print('d2', d2.shape)\n",
    "        d3 = self.down2(d2)\n",
    "#         print('d3', d3.shape)\n",
    "        d4 = self.down3(d3)\n",
    "#         print('d4', d4.shape)\n",
    "        d5 = self.down4(d4)\n",
    "#         print('d5', d5.shape)\n",
    "        d6 = self.down5(d5)\n",
    "#         print('d6', d6.shape)\n",
    "        bottleneck = self.bottleneck(d6)\n",
    "#         print('bottleneck', bottleneck.shape)\n",
    "        up1 = self.up1(bottleneck)\n",
    "#         print('up1', up1.shape)\n",
    "        if up1.shape != d6.shape:\n",
    "            up1 = TF.resize(up1, size=d6.shape[1:]) \n",
    "        up2 = self.up2(torch.cat([up1, d6], 1))\n",
    "#         print('up2', up2.shape)\n",
    "        if up2.shape != d5.shape:\n",
    "            up2 = TF.resize(up2, size=d5.shape[1:])\n",
    "        up3 = self.up3(torch.cat([up2, d5], 1))\n",
    "#         print('up3', up3.shape)\n",
    "        if up3.shape != d4.shape:\n",
    "            up3 = TF.resize(up3, size=d4.shape[1:])\n",
    "        up4 = self.up4(torch.cat([up3, d4], 1))\n",
    "#         print('up4', up4.shape)\n",
    "        if up4.shape != d3.shape:\n",
    "            up4 = TF.resize(up4, size=d3.shape[1:])\n",
    "        up5 = self.up5(torch.cat([up4, d3], 1))\n",
    "#         print('up5', up5.shape)\n",
    "        if up5.shape != d2.shape:\n",
    "            up5 = TF.resize(up5, size=d2.shape[1:])\n",
    "        up6 = self.up6(torch.cat([up5, d2], 1))\n",
    "#         print('up6', up6.shape)\n",
    "        if up6.shape != d1.shape:\n",
    "            up6 = TF.resize(up6, size=d1.shape[1:])\n",
    "        return self.final_up(torch.cat([up6, d1], 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 5, 64])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 5, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "model_gen = Generator()\n",
    "for i, data in enumerate(train_loader):\n",
    "    x, y = data\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    out = model_gen(x)\n",
    "    break\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "q11WRqh0XGuO"
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"\n",
    "            ),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=len(required_params), features=[32, 64, 128, 256]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels ,\n",
    "                features[0],\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2),\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(\n",
    "            nn.Conv1d(\n",
    "                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n",
    "            ),\n",
    "        )\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "model_disk = Discriminator()\n",
    "for i, data in enumerate(train_loader):\n",
    "    x, y = data\n",
    "    out = model_disk(x)\n",
    "    break\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inception time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inceptiontime.src.models.inception import InceptionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inception = InceptionModel(num_blocks=2, in_channels=len(required_params), \n",
    "                       out_channels=30,\n",
    "                       bottleneck_channels=12, kernel_sizes=15,\n",
    "                       use_residuals=True, \n",
    "                       num_pred_classes=len(wells)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_inception.parameters(), lr=1e-3)\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_state_dict = None\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 3.035, Val loss: 2.993, Train acc: 0.084, Val acc: 0.105, \n",
      "Epoch: 2, Train loss: 2.921, Val loss: 2.87, Train acc: 0.107, Val acc: 0.12, \n",
      "Epoch: 3, Train loss: 2.84, Val loss: 2.846, Train acc: 0.118, Val acc: 0.12, \n",
      "Epoch: 4, Train loss: 2.808, Val loss: 2.801, Train acc: 0.127, Val acc: 0.11, \n",
      "Epoch: 5, Train loss: 2.796, Val loss: 2.788, Train acc: 0.12, Val acc: 0.121, \n",
      "Epoch: 6, Train loss: 2.771, Val loss: 2.765, Train acc: 0.127, Val acc: 0.117, \n",
      "Epoch: 7, Train loss: 2.755, Val loss: 2.761, Train acc: 0.133, Val acc: 0.126, \n",
      "Epoch: 8, Train loss: 2.741, Val loss: 2.794, Train acc: 0.144, Val acc: 0.123, \n",
      "Epoch: 9, Train loss: 2.722, Val loss: 2.738, Train acc: 0.148, Val acc: 0.153, \n",
      "Epoch: 10, Train loss: 2.709, Val loss: 2.748, Train acc: 0.164, Val acc: 0.151, \n",
      "Epoch: 11, Train loss: 2.706, Val loss: 2.727, Train acc: 0.159, Val acc: 0.163, \n",
      "Epoch: 12, Train loss: 2.685, Val loss: 2.732, Train acc: 0.171, Val acc: 0.149, \n",
      "Epoch: 13, Train loss: 2.677, Val loss: 2.696, Train acc: 0.163, Val acc: 0.168, \n",
      "Epoch: 14, Train loss: 2.673, Val loss: 2.706, Train acc: 0.162, Val acc: 0.164, \n",
      "Epoch: 15, Train loss: 2.656, Val loss: 2.689, Train acc: 0.162, Val acc: 0.165, \n",
      "Epoch: 16, Train loss: 2.653, Val loss: 2.674, Train acc: 0.174, Val acc: 0.165, \n",
      "Epoch: 17, Train loss: 2.628, Val loss: 2.675, Train acc: 0.174, Val acc: 0.168, \n",
      "Epoch: 18, Train loss: 2.62, Val loss: 2.663, Train acc: 0.184, Val acc: 0.171, \n",
      "Epoch: 19, Train loss: 2.591, Val loss: 2.693, Train acc: 0.193, Val acc: 0.163, \n",
      "Epoch: 20, Train loss: 2.604, Val loss: 2.632, Train acc: 0.194, Val acc: 0.201, \n",
      "Epoch: 21, Train loss: 2.577, Val loss: 2.635, Train acc: 0.193, Val acc: 0.202, \n",
      "Epoch: 22, Train loss: 2.55, Val loss: 2.6, Train acc: 0.2, Val acc: 0.194, \n",
      "Epoch: 23, Train loss: 2.547, Val loss: 2.627, Train acc: 0.202, Val acc: 0.198, \n",
      "Epoch: 24, Train loss: 2.535, Val loss: 2.608, Train acc: 0.202, Val acc: 0.204, \n",
      "Epoch: 25, Train loss: 2.524, Val loss: 2.594, Train acc: 0.206, Val acc: 0.181, \n",
      "Epoch: 26, Train loss: 2.505, Val loss: 2.561, Train acc: 0.209, Val acc: 0.208, \n",
      "Epoch: 27, Train loss: 2.51, Val loss: 2.551, Train acc: 0.21, Val acc: 0.245, \n",
      "Epoch: 28, Train loss: 2.499, Val loss: 2.584, Train acc: 0.213, Val acc: 0.215, \n",
      "Epoch: 29, Train loss: 2.466, Val loss: 2.541, Train acc: 0.217, Val acc: 0.204, \n",
      "Epoch: 30, Train loss: 2.462, Val loss: 2.575, Train acc: 0.234, Val acc: 0.204, \n",
      "Epoch: 31, Train loss: 2.445, Val loss: 2.571, Train acc: 0.224, Val acc: 0.192, \n",
      "Epoch: 32, Train loss: 2.438, Val loss: 2.515, Train acc: 0.227, Val acc: 0.217, \n",
      "Epoch: 33, Train loss: 2.415, Val loss: 2.52, Train acc: 0.235, Val acc: 0.222, \n",
      "Epoch: 34, Train loss: 2.41, Val loss: 2.526, Train acc: 0.237, Val acc: 0.208, \n",
      "Epoch: 35, Train loss: 2.416, Val loss: 2.524, Train acc: 0.232, Val acc: 0.202, \n",
      "Epoch: 36, Train loss: 2.394, Val loss: 2.494, Train acc: 0.233, Val acc: 0.225, \n",
      "Epoch: 37, Train loss: 2.377, Val loss: 2.503, Train acc: 0.247, Val acc: 0.223, \n",
      "Epoch: 38, Train loss: 2.365, Val loss: 2.457, Train acc: 0.239, Val acc: 0.218, \n",
      "Epoch: 39, Train loss: 2.361, Val loss: 2.504, Train acc: 0.248, Val acc: 0.215, \n",
      "Epoch: 40, Train loss: 2.34, Val loss: 2.527, Train acc: 0.256, Val acc: 0.214, \n",
      "Epoch: 41, Train loss: 2.341, Val loss: 2.504, Train acc: 0.251, Val acc: 0.231, \n",
      "Epoch: 42, Train loss: 2.329, Val loss: 2.466, Train acc: 0.252, Val acc: 0.228, \n",
      "Epoch: 43, Train loss: 2.318, Val loss: 2.475, Train acc: 0.255, Val acc: 0.243, \n",
      "Epoch: 44, Train loss: 2.309, Val loss: 2.519, Train acc: 0.267, Val acc: 0.209, \n",
      "Epoch: 45, Train loss: 2.301, Val loss: 2.487, Train acc: 0.266, Val acc: 0.218, \n",
      "Epoch: 46, Train loss: 2.295, Val loss: 2.481, Train acc: 0.263, Val acc: 0.227, \n",
      "Epoch: 47, Train loss: 2.295, Val loss: 2.543, Train acc: 0.268, Val acc: 0.201, \n",
      "Epoch: 48, Train loss: 2.288, Val loss: 2.476, Train acc: 0.255, Val acc: 0.208, \n",
      "Epoch: 49, Train loss: 2.273, Val loss: 2.433, Train acc: 0.269, Val acc: 0.238, \n",
      "Epoch: 50, Train loss: 2.271, Val loss: 2.451, Train acc: 0.267, Val acc: 0.218, \n",
      "Epoch: 51, Train loss: 2.267, Val loss: 2.42, Train acc: 0.267, Val acc: 0.248, \n",
      "Epoch: 52, Train loss: 2.251, Val loss: 2.496, Train acc: 0.273, Val acc: 0.221, \n",
      "Epoch: 53, Train loss: 2.249, Val loss: 2.431, Train acc: 0.28, Val acc: 0.256, \n",
      "Epoch: 54, Train loss: 2.241, Val loss: 2.41, Train acc: 0.281, Val acc: 0.237, \n",
      "Epoch: 55, Train loss: 2.24, Val loss: 2.492, Train acc: 0.28, Val acc: 0.225, \n",
      "Epoch: 56, Train loss: 2.229, Val loss: 2.358, Train acc: 0.282, Val acc: 0.253, \n",
      "Epoch: 57, Train loss: 2.222, Val loss: 2.404, Train acc: 0.28, Val acc: 0.231, \n",
      "Epoch: 58, Train loss: 2.233, Val loss: 2.35, Train acc: 0.273, Val acc: 0.259, \n",
      "Epoch: 59, Train loss: 2.221, Val loss: 2.455, Train acc: 0.281, Val acc: 0.205, \n",
      "Epoch: 60, Train loss: 2.193, Val loss: 2.424, Train acc: 0.289, Val acc: 0.245, \n",
      "Epoch: 61, Train loss: 2.2, Val loss: 2.367, Train acc: 0.288, Val acc: 0.26, \n",
      "Epoch: 62, Train loss: 2.183, Val loss: 2.328, Train acc: 0.299, Val acc: 0.27, \n",
      "Epoch: 63, Train loss: 2.186, Val loss: 2.368, Train acc: 0.292, Val acc: 0.254, \n",
      "Epoch: 64, Train loss: 2.171, Val loss: 2.342, Train acc: 0.297, Val acc: 0.258, \n",
      "Epoch: 65, Train loss: 2.185, Val loss: 2.407, Train acc: 0.299, Val acc: 0.233, \n",
      "Epoch: 66, Train loss: 2.157, Val loss: 2.389, Train acc: 0.298, Val acc: 0.25, \n",
      "Epoch: 67, Train loss: 2.154, Val loss: 2.325, Train acc: 0.303, Val acc: 0.265, \n",
      "Epoch: 68, Train loss: 2.146, Val loss: 2.399, Train acc: 0.306, Val acc: 0.254, \n",
      "Epoch: 69, Train loss: 2.144, Val loss: 2.426, Train acc: 0.311, Val acc: 0.238, \n",
      "Epoch: 70, Train loss: 2.154, Val loss: 2.4, Train acc: 0.298, Val acc: 0.244, \n",
      "Epoch: 71, Train loss: 2.13, Val loss: 2.435, Train acc: 0.313, Val acc: 0.226, \n",
      "Epoch: 72, Train loss: 2.127, Val loss: 2.447, Train acc: 0.311, Val acc: 0.238, \n",
      "Epoch: 73, Train loss: 2.139, Val loss: 2.304, Train acc: 0.31, Val acc: 0.277, \n",
      "Epoch: 74, Train loss: 2.13, Val loss: 2.366, Train acc: 0.311, Val acc: 0.258, \n",
      "Epoch: 75, Train loss: 2.112, Val loss: 2.263, Train acc: 0.309, Val acc: 0.292, \n",
      "Epoch: 76, Train loss: 2.12, Val loss: 2.283, Train acc: 0.317, Val acc: 0.29, \n",
      "Epoch: 77, Train loss: 2.105, Val loss: 2.32, Train acc: 0.323, Val acc: 0.273, \n",
      "Epoch: 78, Train loss: 2.114, Val loss: 2.293, Train acc: 0.315, Val acc: 0.263, \n",
      "Epoch: 79, Train loss: 2.09, Val loss: 2.357, Train acc: 0.32, Val acc: 0.261, \n",
      "Epoch: 80, Train loss: 2.106, Val loss: 2.444, Train acc: 0.322, Val acc: 0.237, \n",
      "Epoch: 81, Train loss: 2.095, Val loss: 2.329, Train acc: 0.328, Val acc: 0.253, \n",
      "Epoch: 82, Train loss: 2.083, Val loss: 2.338, Train acc: 0.334, Val acc: 0.253, \n",
      "Epoch: 83, Train loss: 2.101, Val loss: 2.305, Train acc: 0.308, Val acc: 0.273, \n",
      "Epoch: 84, Train loss: 2.071, Val loss: 2.295, Train acc: 0.331, Val acc: 0.265, \n",
      "Epoch: 85, Train loss: 2.072, Val loss: 2.256, Train acc: 0.323, Val acc: 0.291, \n",
      "Epoch: 86, Train loss: 2.073, Val loss: 2.359, Train acc: 0.329, Val acc: 0.26, \n",
      "Epoch: 87, Train loss: 2.058, Val loss: 2.326, Train acc: 0.331, Val acc: 0.293, \n",
      "Epoch: 88, Train loss: 2.034, Val loss: 2.279, Train acc: 0.336, Val acc: 0.287, \n",
      "Epoch: 89, Train loss: 2.042, Val loss: 2.317, Train acc: 0.337, Val acc: 0.273, \n",
      "Epoch: 90, Train loss: 2.047, Val loss: 2.369, Train acc: 0.336, Val acc: 0.243, \n",
      "Epoch: 91, Train loss: 2.035, Val loss: 2.436, Train acc: 0.341, Val acc: 0.241, \n",
      "Epoch: 92, Train loss: 2.039, Val loss: 2.298, Train acc: 0.321, Val acc: 0.268, \n",
      "Epoch: 93, Train loss: 2.032, Val loss: 2.219, Train acc: 0.342, Val acc: 0.311, \n",
      "Epoch: 94, Train loss: 2.019, Val loss: 2.397, Train acc: 0.339, Val acc: 0.244, \n",
      "Epoch: 95, Train loss: 2.006, Val loss: 2.414, Train acc: 0.344, Val acc: 0.237, \n",
      "Epoch: 96, Train loss: 2.006, Val loss: 2.271, Train acc: 0.336, Val acc: 0.286, \n",
      "Epoch: 97, Train loss: 1.991, Val loss: 2.398, Train acc: 0.355, Val acc: 0.24, \n",
      "Epoch: 98, Train loss: 1.986, Val loss: 2.374, Train acc: 0.355, Val acc: 0.261, \n",
      "Epoch: 99, Train loss: 2.007, Val loss: 2.313, Train acc: 0.346, Val acc: 0.235, \n",
      "Epoch: 100, Train loss: 1.988, Val loss: 2.413, Train acc: 0.355, Val acc: 0.233, \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model_inception.train();\n",
    "    epoch_train_loss = []\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for x_t, y_t in train_loader:\n",
    "        x_t = x_t.cuda()\n",
    "        y_t = y_t.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model_inception(x_t)\n",
    "        loss = criterion(output, y_t)\n",
    "        epoch_train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, preds = torch.max(output, 1)\n",
    "        all_preds.append(preds)\n",
    "        all_y.append(y_t)\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    train_acc.append(float((torch.cat(all_preds) == torch.cat(all_y)).float().mean().cpu().data.numpy()))\n",
    "    epoch_val_loss = []\n",
    "    model_inception.eval()\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for x_v, y_v in val_loader:\n",
    "        x_v = x_v.cuda()\n",
    "        y_v = y_v.cuda()\n",
    "        with torch.no_grad():\n",
    "            output = model_inception(x_v)\n",
    "            loss = criterion(output, y_v)\n",
    "            epoch_val_loss.append(loss.item())\n",
    "            _, preds = torch.max(output, 1)\n",
    "            all_preds.append(preds)\n",
    "            all_y.append(y_v)\n",
    "    val_loss.append(np.mean(epoch_val_loss))\n",
    "    val_acc.append(float((torch.cat(all_preds) == torch.cat(all_y)).float().mean().cpu().data.numpy()))\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, '\n",
    "          f'Train loss: {round(train_loss[-1], 3)}, '\n",
    "          f'Val loss: {round(val_loss[-1], 3)}, '\n",
    "          f'Train acc: {round(train_acc[-1], 3)}, '\n",
    "          f'Val acc: {round(val_acc[-1], 3)}, ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPuWwI7NbSOl"
   },
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "iZNAeQER9Gpy"
   },
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "n_noise = 256\n",
    "n_critic = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "cz1BxWxJB6QR"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)\n",
    "\n",
    "criterion_gan = nn.BCELoss()\n",
    "criterion_soft_dtw = SoftDTW(use_cuda=True, gamma=0.1)\n",
    "D_opt = torch.optim.Adam(discriminator.parameters(), lr = lr)\n",
    "G_opt = torch.optim.Adam(generator.parameters(), lr = lr)\n",
    "# Soft_dtw_opt = torch.optim.Adam(generator.parameters(), lr = lr)\n",
    "fixed_noise = torch.randn((batch_size, len(required_params), 64)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator paramteres: 174625\n",
      "Generator paramteres: 2419237\n"
     ]
    }
   ],
   "source": [
    "print('Discriminator paramteres:', sum(p.numel() for p in discriminator.parameters() if p.requires_grad))\n",
    "print('Generator paramteres:', sum(p.numel() for p in generator.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "BWrDGBzZ9pZQ"
   },
   "outputs": [],
   "source": [
    "writer_fake = SummaryWriter(f\"/home/sharifullina/thesis/logs_cGAN_time_series/fake\")\n",
    "writer_real = SummaryWriter(f\"/home/sharifullina/thesis/logs_cGAN_time_series/real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "img_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] Batch 0/63             Loss D: 0.7087, loss G: 0.8210\n",
      "Epoch [0/100] Batch 10/63             Loss D: 0.6970, loss G: 0.9057\n",
      "Epoch [0/100] Batch 20/63             Loss D: 0.6012, loss G: 1.1300\n",
      "Epoch [0/100] Batch 30/63             Loss D: 0.6021, loss G: 1.2970\n",
      "Epoch [0/100] Batch 40/63             Loss D: 0.5811, loss G: 1.4607\n",
      "Epoch [0/100] Batch 50/63             Loss D: 0.4808, loss G: 1.5532\n",
      "Epoch [0/100] Batch 60/63             Loss D: 0.5311, loss G: 2.0244\n",
      "Epoch [1/100] Batch 0/63             Loss D: 0.3858, loss G: 2.0714\n",
      "Epoch [1/100] Batch 10/63             Loss D: 0.3379, loss G: 2.1392\n",
      "Epoch [1/100] Batch 20/63             Loss D: 0.2464, loss G: 1.8610\n",
      "Epoch [1/100] Batch 30/63             Loss D: 0.2460, loss G: 2.0095\n",
      "Epoch [1/100] Batch 40/63             Loss D: 0.3023, loss G: 1.8875\n",
      "Epoch [1/100] Batch 50/63             Loss D: 0.5199, loss G: 1.5594\n",
      "Epoch [1/100] Batch 60/63             Loss D: 0.5229, loss G: 1.8127\n",
      "Epoch [2/100] Batch 0/63             Loss D: 0.7244, loss G: 1.9048\n",
      "Epoch [2/100] Batch 10/63             Loss D: 0.5820, loss G: 2.2200\n",
      "Epoch [2/100] Batch 20/63             Loss D: 0.2896, loss G: 2.1491\n",
      "Epoch [2/100] Batch 30/63             Loss D: 0.4881, loss G: 1.7858\n",
      "Epoch [2/100] Batch 40/63             Loss D: 0.3111, loss G: 1.7409\n",
      "Epoch [2/100] Batch 50/63             Loss D: 0.4752, loss G: 1.6833\n",
      "Epoch [2/100] Batch 60/63             Loss D: 0.4153, loss G: 1.9047\n",
      "Epoch [3/100] Batch 0/63             Loss D: 0.3269, loss G: 1.9347\n",
      "Epoch [3/100] Batch 10/63             Loss D: 0.4398, loss G: 2.1908\n",
      "Epoch [3/100] Batch 20/63             Loss D: 0.3905, loss G: 2.0484\n",
      "Epoch [3/100] Batch 30/63             Loss D: 0.2675, loss G: 1.6799\n",
      "Epoch [3/100] Batch 40/63             Loss D: 0.2219, loss G: 1.9041\n",
      "Epoch [3/100] Batch 50/63             Loss D: 0.4155, loss G: 1.9512\n",
      "Epoch [3/100] Batch 60/63             Loss D: 0.1881, loss G: 2.0622\n",
      "Epoch [4/100] Batch 0/63             Loss D: 0.1632, loss G: 2.1098\n",
      "Epoch [4/100] Batch 10/63             Loss D: 0.4731, loss G: 1.7324\n",
      "Epoch [4/100] Batch 20/63             Loss D: 0.4544, loss G: 1.8421\n",
      "Epoch [4/100] Batch 30/63             Loss D: 0.2115, loss G: 1.9775\n",
      "Epoch [4/100] Batch 40/63             Loss D: 0.1418, loss G: 2.1758\n",
      "Epoch [4/100] Batch 50/63             Loss D: 0.1881, loss G: 1.9483\n",
      "Epoch [4/100] Batch 60/63             Loss D: 0.1620, loss G: 2.2076\n",
      "Epoch [5/100] Batch 0/63             Loss D: 0.4420, loss G: 2.0498\n",
      "Epoch [5/100] Batch 10/63             Loss D: 0.1299, loss G: 2.4939\n",
      "Epoch [5/100] Batch 20/63             Loss D: 0.1416, loss G: 2.6363\n",
      "Epoch [5/100] Batch 30/63             Loss D: 0.1194, loss G: 2.4694\n",
      "Epoch [5/100] Batch 40/63             Loss D: 0.1169, loss G: 2.5980\n",
      "Epoch [5/100] Batch 50/63             Loss D: 0.1367, loss G: 3.1316\n",
      "Epoch [5/100] Batch 60/63             Loss D: 0.0750, loss G: 2.8316\n",
      "Epoch [6/100] Batch 0/63             Loss D: 0.1177, loss G: 2.5912\n",
      "Epoch [6/100] Batch 10/63             Loss D: 0.1003, loss G: 2.7733\n",
      "Epoch [6/100] Batch 20/63             Loss D: 0.1114, loss G: 2.8836\n",
      "Epoch [6/100] Batch 30/63             Loss D: 0.1086, loss G: 2.7394\n",
      "Epoch [6/100] Batch 40/63             Loss D: 0.2546, loss G: 2.9335\n",
      "Epoch [6/100] Batch 50/63             Loss D: 0.0668, loss G: 3.0677\n",
      "Epoch [6/100] Batch 60/63             Loss D: 0.0554, loss G: 3.2450\n",
      "Epoch [7/100] Batch 0/63             Loss D: 0.0983, loss G: 3.1742\n",
      "Epoch [7/100] Batch 10/63             Loss D: 0.1262, loss G: 3.2327\n",
      "Epoch [7/100] Batch 20/63             Loss D: 0.0832, loss G: 2.8963\n",
      "Epoch [7/100] Batch 30/63             Loss D: 0.0725, loss G: 3.1630\n",
      "Epoch [7/100] Batch 40/63             Loss D: 0.0969, loss G: 3.3034\n",
      "Epoch [7/100] Batch 50/63             Loss D: 0.0900, loss G: 3.2030\n",
      "Epoch [7/100] Batch 60/63             Loss D: 0.0846, loss G: 3.2782\n",
      "Epoch [8/100] Batch 0/63             Loss D: 0.1343, loss G: 3.4868\n",
      "Epoch [8/100] Batch 10/63             Loss D: 0.0694, loss G: 3.2343\n",
      "Epoch [8/100] Batch 20/63             Loss D: 0.0802, loss G: 3.6217\n",
      "Epoch [8/100] Batch 30/63             Loss D: 0.0505, loss G: 3.8200\n",
      "Epoch [8/100] Batch 40/63             Loss D: 0.0551, loss G: 3.6724\n",
      "Epoch [8/100] Batch 50/63             Loss D: 0.0970, loss G: 3.6142\n",
      "Epoch [8/100] Batch 60/63             Loss D: 0.1450, loss G: 3.0792\n",
      "Epoch [9/100] Batch 0/63             Loss D: 0.0571, loss G: 3.4997\n",
      "Epoch [9/100] Batch 10/63             Loss D: 0.0574, loss G: 3.8508\n",
      "Epoch [9/100] Batch 20/63             Loss D: 0.0375, loss G: 3.7452\n",
      "Epoch [9/100] Batch 30/63             Loss D: 0.0362, loss G: 3.4809\n",
      "Epoch [9/100] Batch 40/63             Loss D: 0.0447, loss G: 3.6187\n",
      "Epoch [9/100] Batch 50/63             Loss D: 0.0280, loss G: 4.0416\n",
      "Epoch [9/100] Batch 60/63             Loss D: 0.1068, loss G: 4.0840\n",
      "Epoch [10/100] Batch 0/63             Loss D: 0.0264, loss G: 4.0910\n",
      "Epoch [10/100] Batch 10/63             Loss D: 0.0311, loss G: 3.6330\n",
      "Epoch [10/100] Batch 20/63             Loss D: 0.0333, loss G: 3.6674\n",
      "Epoch [10/100] Batch 30/63             Loss D: 0.0391, loss G: 3.4521\n",
      "Epoch [10/100] Batch 40/63             Loss D: 0.0273, loss G: 3.7203\n",
      "Epoch [10/100] Batch 50/63             Loss D: 0.0327, loss G: 4.0630\n",
      "Epoch [10/100] Batch 60/63             Loss D: 0.0293, loss G: 4.0840\n",
      "Epoch [11/100] Batch 0/63             Loss D: 0.0345, loss G: 4.1339\n",
      "Epoch [11/100] Batch 10/63             Loss D: 0.0891, loss G: 4.3435\n",
      "Epoch [11/100] Batch 20/63             Loss D: 0.0412, loss G: 4.5636\n",
      "Epoch [11/100] Batch 30/63             Loss D: 0.0201, loss G: 4.5818\n",
      "Epoch [11/100] Batch 40/63             Loss D: 0.0200, loss G: 4.6121\n",
      "Epoch [11/100] Batch 50/63             Loss D: 0.0129, loss G: 4.9303\n",
      "Epoch [11/100] Batch 60/63             Loss D: 0.0729, loss G: 4.4241\n",
      "Epoch [12/100] Batch 0/63             Loss D: 0.0257, loss G: 3.8793\n",
      "Epoch [12/100] Batch 10/63             Loss D: 0.0272, loss G: 4.2382\n",
      "Epoch [12/100] Batch 20/63             Loss D: 0.0227, loss G: 4.7704\n",
      "Epoch [12/100] Batch 30/63             Loss D: 0.0276, loss G: 4.5354\n",
      "Epoch [12/100] Batch 40/63             Loss D: 0.0182, loss G: 4.3619\n",
      "Epoch [12/100] Batch 50/63             Loss D: 0.0188, loss G: 4.6584\n",
      "Epoch [12/100] Batch 60/63             Loss D: 0.0269, loss G: 4.3057\n",
      "Epoch [13/100] Batch 0/63             Loss D: 0.0212, loss G: 4.3380\n",
      "Epoch [13/100] Batch 10/63             Loss D: 0.0217, loss G: 3.9996\n",
      "Epoch [13/100] Batch 20/63             Loss D: 0.0260, loss G: 4.0067\n",
      "Epoch [13/100] Batch 30/63             Loss D: 0.0402, loss G: 3.7197\n",
      "Epoch [13/100] Batch 40/63             Loss D: 0.0202, loss G: 4.0331\n",
      "Epoch [13/100] Batch 50/63             Loss D: 0.0129, loss G: 4.6978\n",
      "Epoch [13/100] Batch 60/63             Loss D: 0.0147, loss G: 4.7396\n",
      "Epoch [14/100] Batch 0/63             Loss D: 0.0138, loss G: 4.8516\n",
      "Epoch [14/100] Batch 10/63             Loss D: 0.0178, loss G: 4.5640\n",
      "Epoch [14/100] Batch 20/63             Loss D: 0.0474, loss G: 4.3348\n",
      "Epoch [14/100] Batch 30/63             Loss D: 0.0180, loss G: 4.6413\n",
      "Epoch [14/100] Batch 40/63             Loss D: 0.0113, loss G: 4.6707\n",
      "Epoch [14/100] Batch 50/63             Loss D: 0.0117, loss G: 4.7256\n",
      "Epoch [14/100] Batch 60/63             Loss D: 0.0139, loss G: 4.7098\n",
      "Epoch [15/100] Batch 0/63             Loss D: 0.0120, loss G: 4.7869\n",
      "Epoch [15/100] Batch 10/63             Loss D: 0.0141, loss G: 4.9769\n",
      "Epoch [15/100] Batch 20/63             Loss D: 0.0145, loss G: 5.0904\n",
      "Epoch [15/100] Batch 30/63             Loss D: 0.0403, loss G: 4.5124\n",
      "Epoch [15/100] Batch 40/63             Loss D: 0.0220, loss G: 4.8401\n",
      "Epoch [15/100] Batch 50/63             Loss D: 0.0117, loss G: 5.0371\n",
      "Epoch [15/100] Batch 60/63             Loss D: 0.0095, loss G: 5.1420\n",
      "Epoch [16/100] Batch 0/63             Loss D: 0.0233, loss G: 5.0430\n",
      "Epoch [16/100] Batch 10/63             Loss D: 0.0121, loss G: 5.2601\n",
      "Epoch [16/100] Batch 20/63             Loss D: 0.0115, loss G: 5.3836\n",
      "Epoch [16/100] Batch 30/63             Loss D: 0.0127, loss G: 5.4314\n",
      "Epoch [16/100] Batch 40/63             Loss D: 0.0066, loss G: 5.7336\n",
      "Epoch [16/100] Batch 50/63             Loss D: 0.0084, loss G: 5.4041\n",
      "Epoch [16/100] Batch 60/63             Loss D: 0.0167, loss G: 4.6042\n",
      "Epoch [17/100] Batch 0/63             Loss D: 0.0256, loss G: 3.9083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100] Batch 10/63             Loss D: 0.0226, loss G: 4.4328\n",
      "Epoch [17/100] Batch 20/63             Loss D: 0.0152, loss G: 4.7282\n",
      "Epoch [17/100] Batch 30/63             Loss D: 0.0144, loss G: 4.8506\n",
      "Epoch [17/100] Batch 40/63             Loss D: 0.0257, loss G: 4.9922\n",
      "Epoch [17/100] Batch 50/63             Loss D: 0.0333, loss G: 4.5051\n",
      "Epoch [17/100] Batch 60/63             Loss D: 0.0276, loss G: 4.5137\n",
      "Epoch [18/100] Batch 0/63             Loss D: 0.0196, loss G: 4.6255\n",
      "Epoch [18/100] Batch 10/63             Loss D: 0.0493, loss G: 4.4018\n",
      "Epoch [18/100] Batch 20/63             Loss D: 0.0209, loss G: 4.8589\n",
      "Epoch [18/100] Batch 30/63             Loss D: 0.0131, loss G: 4.8772\n",
      "Epoch [18/100] Batch 40/63             Loss D: 0.0178, loss G: 4.8081\n",
      "Epoch [18/100] Batch 50/63             Loss D: 0.0414, loss G: 4.9904\n",
      "Epoch [18/100] Batch 60/63             Loss D: 0.0128, loss G: 5.4537\n",
      "Epoch [19/100] Batch 0/63             Loss D: 0.0125, loss G: 5.6813\n",
      "Epoch [19/100] Batch 10/63             Loss D: 0.0118, loss G: 5.1452\n",
      "Epoch [19/100] Batch 20/63             Loss D: 0.0291, loss G: 5.3764\n",
      "Epoch [19/100] Batch 30/63             Loss D: 0.0288, loss G: 5.0766\n",
      "Epoch [19/100] Batch 40/63             Loss D: 0.0088, loss G: 5.5252\n",
      "Epoch [19/100] Batch 50/63             Loss D: 0.0098, loss G: 5.1883\n",
      "Epoch [19/100] Batch 60/63             Loss D: 0.0262, loss G: 5.1657\n",
      "Epoch [20/100] Batch 0/63             Loss D: 0.0355, loss G: 5.5090\n",
      "Epoch [20/100] Batch 10/63             Loss D: 0.0181, loss G: 5.5584\n",
      "Epoch [20/100] Batch 20/63             Loss D: 0.0162, loss G: 5.0376\n",
      "Epoch [20/100] Batch 30/63             Loss D: 0.0088, loss G: 5.4584\n",
      "Epoch [20/100] Batch 40/63             Loss D: 0.0113, loss G: 5.1857\n",
      "Epoch [20/100] Batch 50/63             Loss D: 0.0309, loss G: 5.5856\n",
      "Epoch [20/100] Batch 60/63             Loss D: 0.0130, loss G: 5.4759\n",
      "Epoch [21/100] Batch 0/63             Loss D: 0.0097, loss G: 5.5747\n",
      "Epoch [21/100] Batch 10/63             Loss D: 0.0137, loss G: 5.4029\n",
      "Epoch [21/100] Batch 20/63             Loss D: 0.0120, loss G: 4.9823\n",
      "Epoch [21/100] Batch 30/63             Loss D: 0.0116, loss G: 5.4585\n",
      "Epoch [21/100] Batch 40/63             Loss D: 0.0142, loss G: 5.1604\n",
      "Epoch [21/100] Batch 50/63             Loss D: 0.0133, loss G: 5.5930\n",
      "Epoch [21/100] Batch 60/63             Loss D: 0.0115, loss G: 5.2387\n",
      "Epoch [22/100] Batch 0/63             Loss D: 0.0086, loss G: 5.6783\n",
      "Epoch [22/100] Batch 10/63             Loss D: 0.0269, loss G: 5.0239\n",
      "Epoch [22/100] Batch 20/63             Loss D: 0.0099, loss G: 5.7899\n",
      "Epoch [22/100] Batch 30/63             Loss D: 0.0146, loss G: 5.4362\n",
      "Epoch [22/100] Batch 40/63             Loss D: 0.0133, loss G: 5.3031\n",
      "Epoch [22/100] Batch 50/63             Loss D: 0.0144, loss G: 5.1001\n",
      "Epoch [22/100] Batch 60/63             Loss D: 0.0216, loss G: 4.8509\n",
      "Epoch [23/100] Batch 0/63             Loss D: 0.0118, loss G: 5.3244\n",
      "Epoch [23/100] Batch 10/63             Loss D: 0.0166, loss G: 5.2824\n",
      "Epoch [23/100] Batch 20/63             Loss D: 0.0176, loss G: 5.8084\n",
      "Epoch [23/100] Batch 30/63             Loss D: 0.0154, loss G: 6.0052\n",
      "Epoch [23/100] Batch 40/63             Loss D: 0.0364, loss G: 6.0068\n",
      "Epoch [23/100] Batch 50/63             Loss D: 0.0127, loss G: 5.5458\n",
      "Epoch [23/100] Batch 60/63             Loss D: 0.0101, loss G: 5.2178\n",
      "Epoch [24/100] Batch 0/63             Loss D: 0.0080, loss G: 5.3882\n",
      "Epoch [24/100] Batch 10/63             Loss D: 0.0289, loss G: 5.6538\n",
      "Epoch [24/100] Batch 20/63             Loss D: 0.0109, loss G: 5.2110\n",
      "Epoch [24/100] Batch 30/63             Loss D: 0.0385, loss G: 5.5137\n",
      "Epoch [24/100] Batch 40/63             Loss D: 0.0150, loss G: 5.4923\n",
      "Epoch [24/100] Batch 50/63             Loss D: 0.0385, loss G: 5.6633\n",
      "Epoch [24/100] Batch 60/63             Loss D: 0.0142, loss G: 5.2275\n",
      "Epoch [25/100] Batch 0/63             Loss D: 0.0117, loss G: 5.3549\n",
      "Epoch [25/100] Batch 10/63             Loss D: 0.0098, loss G: 5.6416\n",
      "Epoch [25/100] Batch 20/63             Loss D: 0.0146, loss G: 5.4062\n",
      "Epoch [25/100] Batch 30/63             Loss D: 0.0293, loss G: 5.6833\n",
      "Epoch [25/100] Batch 40/63             Loss D: 0.0094, loss G: 5.0689\n",
      "Epoch [25/100] Batch 50/63             Loss D: 0.0083, loss G: 5.0117\n",
      "Epoch [25/100] Batch 60/63             Loss D: 0.0110, loss G: 5.1526\n",
      "Epoch [26/100] Batch 0/63             Loss D: 0.0294, loss G: 4.8364\n",
      "Epoch [26/100] Batch 10/63             Loss D: 0.0101, loss G: 5.0932\n",
      "Epoch [26/100] Batch 20/63             Loss D: 0.0076, loss G: 5.9268\n",
      "Epoch [26/100] Batch 30/63             Loss D: 0.0121, loss G: 4.8590\n",
      "Epoch [26/100] Batch 40/63             Loss D: 0.0187, loss G: 5.9374\n",
      "Epoch [26/100] Batch 50/63             Loss D: 0.0142, loss G: 5.3356\n",
      "Epoch [26/100] Batch 60/63             Loss D: 0.0137, loss G: 5.3207\n",
      "Epoch [27/100] Batch 0/63             Loss D: 0.0137, loss G: 5.0998\n",
      "Epoch [27/100] Batch 10/63             Loss D: 0.0070, loss G: 5.5063\n",
      "Epoch [27/100] Batch 20/63             Loss D: 0.0075, loss G: 5.4413\n",
      "Epoch [27/100] Batch 30/63             Loss D: 0.0086, loss G: 5.0310\n",
      "Epoch [27/100] Batch 40/63             Loss D: 0.0093, loss G: 5.2327\n",
      "Epoch [27/100] Batch 50/63             Loss D: 0.0062, loss G: 5.2006\n",
      "Epoch [27/100] Batch 60/63             Loss D: 0.0092, loss G: 5.3915\n",
      "Epoch [28/100] Batch 0/63             Loss D: 0.0086, loss G: 5.2632\n",
      "Epoch [28/100] Batch 10/63             Loss D: 0.0100, loss G: 4.6789\n",
      "Epoch [28/100] Batch 20/63             Loss D: 0.0178, loss G: 4.3387\n",
      "Epoch [28/100] Batch 30/63             Loss D: 0.0095, loss G: 5.0518\n",
      "Epoch [28/100] Batch 40/63             Loss D: 0.0063, loss G: 5.5325\n",
      "Epoch [28/100] Batch 50/63             Loss D: 0.0121, loss G: 5.5601\n",
      "Epoch [28/100] Batch 60/63             Loss D: 0.0082, loss G: 5.4454\n",
      "Epoch [29/100] Batch 0/63             Loss D: 0.0167, loss G: 5.7679\n",
      "Epoch [29/100] Batch 10/63             Loss D: 0.0081, loss G: 5.5484\n",
      "Epoch [29/100] Batch 20/63             Loss D: 0.0091, loss G: 5.3191\n",
      "Epoch [29/100] Batch 30/63             Loss D: 0.0073, loss G: 6.1595\n",
      "Epoch [29/100] Batch 40/63             Loss D: 0.0074, loss G: 5.7015\n",
      "Epoch [29/100] Batch 50/63             Loss D: 0.0097, loss G: 5.7860\n",
      "Epoch [29/100] Batch 60/63             Loss D: 0.0109, loss G: 5.8065\n",
      "Epoch [30/100] Batch 0/63             Loss D: 0.0087, loss G: 5.2130\n",
      "Epoch [30/100] Batch 10/63             Loss D: 0.0108, loss G: 5.1371\n",
      "Epoch [30/100] Batch 20/63             Loss D: 0.0091, loss G: 5.2158\n",
      "Epoch [30/100] Batch 30/63             Loss D: 0.0142, loss G: 4.5490\n",
      "Epoch [30/100] Batch 40/63             Loss D: 0.0060, loss G: 6.1747\n",
      "Epoch [30/100] Batch 50/63             Loss D: 0.0123, loss G: 5.4700\n",
      "Epoch [30/100] Batch 60/63             Loss D: 0.0085, loss G: 4.6880\n",
      "Epoch [31/100] Batch 0/63             Loss D: 0.0233, loss G: 4.6878\n",
      "Epoch [31/100] Batch 10/63             Loss D: 0.0186, loss G: 4.9163\n",
      "Epoch [31/100] Batch 20/63             Loss D: 0.0233, loss G: 3.9443\n",
      "Epoch [31/100] Batch 30/63             Loss D: 0.0122, loss G: 5.1260\n",
      "Epoch [31/100] Batch 40/63             Loss D: 0.0233, loss G: 4.7496\n",
      "Epoch [31/100] Batch 50/63             Loss D: 0.0098, loss G: 5.0720\n",
      "Epoch [31/100] Batch 60/63             Loss D: 0.0104, loss G: 4.9599\n",
      "Epoch [32/100] Batch 0/63             Loss D: 0.0132, loss G: 4.2887\n",
      "Epoch [32/100] Batch 10/63             Loss D: 0.0184, loss G: 4.4717\n",
      "Epoch [32/100] Batch 20/63             Loss D: 0.0231, loss G: 5.2322\n",
      "Epoch [32/100] Batch 30/63             Loss D: 0.0168, loss G: 6.0854\n",
      "Epoch [32/100] Batch 40/63             Loss D: 0.0088, loss G: 5.0520\n",
      "Epoch [32/100] Batch 50/63             Loss D: 0.0080, loss G: 5.2366\n",
      "Epoch [32/100] Batch 60/63             Loss D: 0.0353, loss G: 4.7823\n",
      "Epoch [33/100] Batch 0/63             Loss D: 0.0098, loss G: 5.5623\n",
      "Epoch [33/100] Batch 10/63             Loss D: 0.0083, loss G: 5.4192\n",
      "Epoch [33/100] Batch 20/63             Loss D: 0.0100, loss G: 5.1691\n",
      "Epoch [33/100] Batch 30/63             Loss D: 0.0085, loss G: 5.1753\n",
      "Epoch [33/100] Batch 40/63             Loss D: 0.0079, loss G: 5.6348\n",
      "Epoch [33/100] Batch 50/63             Loss D: 0.0121, loss G: 4.8954\n",
      "Epoch [33/100] Batch 60/63             Loss D: 0.0075, loss G: 5.3681\n",
      "Epoch [34/100] Batch 0/63             Loss D: 0.0064, loss G: 5.7599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100] Batch 10/63             Loss D: 0.0092, loss G: 4.9149\n",
      "Epoch [34/100] Batch 20/63             Loss D: 0.0058, loss G: 5.8282\n",
      "Epoch [34/100] Batch 30/63             Loss D: 0.0071, loss G: 5.3305\n",
      "Epoch [34/100] Batch 40/63             Loss D: 0.0057, loss G: 5.5495\n",
      "Epoch [34/100] Batch 50/63             Loss D: 0.0156, loss G: 5.3345\n",
      "Epoch [34/100] Batch 60/63             Loss D: 0.0050, loss G: 5.9687\n",
      "Epoch [35/100] Batch 0/63             Loss D: 0.0046, loss G: 5.8107\n",
      "Epoch [35/100] Batch 10/63             Loss D: 0.0071, loss G: 5.6735\n",
      "Epoch [35/100] Batch 20/63             Loss D: 0.0051, loss G: 5.7086\n",
      "Epoch [35/100] Batch 30/63             Loss D: 0.0046, loss G: 6.0632\n",
      "Epoch [35/100] Batch 40/63             Loss D: 0.0052, loss G: 5.8623\n",
      "Epoch [35/100] Batch 50/63             Loss D: 0.0051, loss G: 5.9848\n",
      "Epoch [35/100] Batch 60/63             Loss D: 0.0053, loss G: 6.3103\n",
      "Epoch [36/100] Batch 0/63             Loss D: 0.0039, loss G: 6.4741\n",
      "Epoch [36/100] Batch 10/63             Loss D: 0.0052, loss G: 6.4888\n",
      "Epoch [36/100] Batch 20/63             Loss D: 0.0058, loss G: 5.7578\n",
      "Epoch [36/100] Batch 30/63             Loss D: 0.0067, loss G: 5.5599\n",
      "Epoch [36/100] Batch 40/63             Loss D: 0.0110, loss G: 5.5981\n",
      "Epoch [36/100] Batch 50/63             Loss D: 0.0056, loss G: 5.6874\n",
      "Epoch [36/100] Batch 60/63             Loss D: 0.0041, loss G: 6.3693\n",
      "Epoch [37/100] Batch 0/63             Loss D: 0.0089, loss G: 5.6248\n",
      "Epoch [37/100] Batch 10/63             Loss D: 0.0051, loss G: 5.5425\n",
      "Epoch [37/100] Batch 20/63             Loss D: 0.0048, loss G: 5.6433\n",
      "Epoch [37/100] Batch 30/63             Loss D: 0.0450, loss G: 6.1774\n",
      "Epoch [37/100] Batch 40/63             Loss D: 0.0054, loss G: 5.6001\n",
      "Epoch [37/100] Batch 50/63             Loss D: 0.0053, loss G: 5.7985\n",
      "Epoch [37/100] Batch 60/63             Loss D: 0.0048, loss G: 5.6875\n",
      "Epoch [38/100] Batch 0/63             Loss D: 0.0045, loss G: 5.7604\n",
      "Epoch [38/100] Batch 10/63             Loss D: 0.0051, loss G: 5.4991\n",
      "Epoch [38/100] Batch 20/63             Loss D: 0.0105, loss G: 5.5332\n",
      "Epoch [38/100] Batch 30/63             Loss D: 0.0068, loss G: 5.8681\n",
      "Epoch [38/100] Batch 40/63             Loss D: 0.0068, loss G: 5.8324\n",
      "Epoch [38/100] Batch 50/63             Loss D: 0.0056, loss G: 5.9407\n",
      "Epoch [38/100] Batch 60/63             Loss D: 0.0065, loss G: 5.9708\n",
      "Epoch [39/100] Batch 0/63             Loss D: 0.0056, loss G: 6.0401\n",
      "Epoch [39/100] Batch 10/63             Loss D: 0.0092, loss G: 5.8271\n",
      "Epoch [39/100] Batch 20/63             Loss D: 0.0063, loss G: 5.8162\n",
      "Epoch [39/100] Batch 30/63             Loss D: 0.0066, loss G: 6.1330\n",
      "Epoch [39/100] Batch 40/63             Loss D: 0.0056, loss G: 5.9149\n",
      "Epoch [39/100] Batch 50/63             Loss D: 0.0045, loss G: 6.0803\n",
      "Epoch [39/100] Batch 60/63             Loss D: 0.0050, loss G: 6.3638\n",
      "Epoch [40/100] Batch 0/63             Loss D: 0.0049, loss G: 5.9927\n",
      "Epoch [40/100] Batch 10/63             Loss D: 0.0057, loss G: 5.7898\n",
      "Epoch [40/100] Batch 20/63             Loss D: 0.0047, loss G: 5.7446\n",
      "Epoch [40/100] Batch 30/63             Loss D: 0.0053, loss G: 6.2012\n",
      "Epoch [40/100] Batch 40/63             Loss D: 0.0094, loss G: 5.9812\n",
      "Epoch [40/100] Batch 50/63             Loss D: 0.0061, loss G: 6.0161\n",
      "Epoch [40/100] Batch 60/63             Loss D: 0.0040, loss G: 6.7258\n",
      "Epoch [41/100] Batch 0/63             Loss D: 0.0042, loss G: 6.4276\n",
      "Epoch [41/100] Batch 10/63             Loss D: 0.0045, loss G: 6.7946\n",
      "Epoch [41/100] Batch 20/63             Loss D: 0.0044, loss G: 6.1814\n",
      "Epoch [41/100] Batch 30/63             Loss D: 0.0071, loss G: 5.7314\n",
      "Epoch [41/100] Batch 40/63             Loss D: 0.0055, loss G: 6.3354\n",
      "Epoch [41/100] Batch 50/63             Loss D: 0.0081, loss G: 6.2527\n",
      "Epoch [41/100] Batch 60/63             Loss D: 0.0061, loss G: 6.3468\n",
      "Epoch [42/100] Batch 0/63             Loss D: 0.0053, loss G: 6.8930\n",
      "Epoch [42/100] Batch 10/63             Loss D: 0.0060, loss G: 6.0839\n",
      "Epoch [42/100] Batch 20/63             Loss D: 0.0046, loss G: 6.4743\n",
      "Epoch [42/100] Batch 30/63             Loss D: 0.0065, loss G: 6.3167\n",
      "Epoch [42/100] Batch 40/63             Loss D: 0.0051, loss G: 6.1543\n",
      "Epoch [42/100] Batch 50/63             Loss D: 0.0067, loss G: 6.0681\n",
      "Epoch [42/100] Batch 60/63             Loss D: 0.0047, loss G: 6.5592\n",
      "Epoch [43/100] Batch 0/63             Loss D: 0.0057, loss G: 6.5673\n",
      "Epoch [43/100] Batch 10/63             Loss D: 0.0060, loss G: 6.1310\n",
      "Epoch [43/100] Batch 20/63             Loss D: 0.0041, loss G: 6.1866\n",
      "Epoch [43/100] Batch 30/63             Loss D: 0.0084, loss G: 6.1119\n",
      "Epoch [43/100] Batch 40/63             Loss D: 0.0068, loss G: 6.5066\n",
      "Epoch [43/100] Batch 50/63             Loss D: 0.0043, loss G: 6.0538\n",
      "Epoch [43/100] Batch 60/63             Loss D: 0.0045, loss G: 6.0323\n",
      "Epoch [44/100] Batch 0/63             Loss D: 0.0037, loss G: 7.1104\n",
      "Epoch [44/100] Batch 10/63             Loss D: 0.0031, loss G: 6.9692\n",
      "Epoch [44/100] Batch 20/63             Loss D: 0.0089, loss G: 6.5459\n",
      "Epoch [44/100] Batch 30/63             Loss D: 0.0040, loss G: 6.1803\n",
      "Epoch [44/100] Batch 40/63             Loss D: 0.0139, loss G: 6.2690\n",
      "Epoch [44/100] Batch 50/63             Loss D: 0.0038, loss G: 6.2340\n",
      "Epoch [44/100] Batch 60/63             Loss D: 0.0049, loss G: 6.8097\n",
      "Epoch [45/100] Batch 0/63             Loss D: 0.0047, loss G: 6.1752\n",
      "Epoch [45/100] Batch 10/63             Loss D: 0.0044, loss G: 6.1426\n",
      "Epoch [45/100] Batch 20/63             Loss D: 0.0035, loss G: 6.7232\n",
      "Epoch [45/100] Batch 30/63             Loss D: 0.0053, loss G: 6.0182\n",
      "Epoch [45/100] Batch 40/63             Loss D: 0.0183, loss G: 6.9170\n",
      "Epoch [45/100] Batch 50/63             Loss D: 0.0042, loss G: 6.9003\n",
      "Epoch [45/100] Batch 60/63             Loss D: 0.0052, loss G: 6.0564\n",
      "Epoch [46/100] Batch 0/63             Loss D: 0.0047, loss G: 6.3738\n",
      "Epoch [46/100] Batch 10/63             Loss D: 0.0045, loss G: 6.3749\n",
      "Epoch [46/100] Batch 20/63             Loss D: 0.0040, loss G: 6.3490\n",
      "Epoch [46/100] Batch 30/63             Loss D: 0.0061, loss G: 7.1397\n",
      "Epoch [46/100] Batch 40/63             Loss D: 0.0035, loss G: 6.5945\n",
      "Epoch [46/100] Batch 50/63             Loss D: 0.0040, loss G: 6.3656\n",
      "Epoch [46/100] Batch 60/63             Loss D: 0.0036, loss G: 7.1983\n",
      "Epoch [47/100] Batch 0/63             Loss D: 0.0037, loss G: 6.6529\n",
      "Epoch [47/100] Batch 10/63             Loss D: 0.0040, loss G: 6.9881\n",
      "Epoch [47/100] Batch 20/63             Loss D: 0.0038, loss G: 7.6458\n",
      "Epoch [47/100] Batch 30/63             Loss D: 0.0037, loss G: 7.0750\n",
      "Epoch [47/100] Batch 40/63             Loss D: 0.0048, loss G: 7.0428\n",
      "Epoch [47/100] Batch 50/63             Loss D: 0.0050, loss G: 6.8429\n",
      "Epoch [47/100] Batch 60/63             Loss D: 0.0038, loss G: 7.7259\n",
      "Epoch [48/100] Batch 0/63             Loss D: 0.0036, loss G: 7.7005\n",
      "Epoch [48/100] Batch 10/63             Loss D: 0.0032, loss G: 6.8595\n",
      "Epoch [48/100] Batch 20/63             Loss D: 0.0039, loss G: 6.8867\n",
      "Epoch [48/100] Batch 30/63             Loss D: 0.0112, loss G: 6.9276\n",
      "Epoch [48/100] Batch 40/63             Loss D: 0.0036, loss G: 7.1916\n",
      "Epoch [48/100] Batch 50/63             Loss D: 0.0037, loss G: 7.7028\n",
      "Epoch [48/100] Batch 60/63             Loss D: 0.0044, loss G: 6.7849\n",
      "Epoch [49/100] Batch 0/63             Loss D: 0.0144, loss G: 7.5359\n",
      "Epoch [49/100] Batch 10/63             Loss D: 0.0041, loss G: 7.0325\n",
      "Epoch [49/100] Batch 20/63             Loss D: 0.0041, loss G: 6.9256\n",
      "Epoch [49/100] Batch 30/63             Loss D: 0.0036, loss G: 7.0119\n",
      "Epoch [49/100] Batch 40/63             Loss D: 0.0033, loss G: 7.4426\n",
      "Epoch [49/100] Batch 50/63             Loss D: 0.0040, loss G: 6.7758\n",
      "Epoch [49/100] Batch 60/63             Loss D: 0.0043, loss G: 6.8758\n",
      "Epoch [50/100] Batch 0/63             Loss D: 0.0032, loss G: 7.8276\n",
      "Epoch [50/100] Batch 10/63             Loss D: 0.0047, loss G: 6.9992\n",
      "Epoch [50/100] Batch 20/63             Loss D: 0.0040, loss G: 7.7597\n",
      "Epoch [50/100] Batch 30/63             Loss D: 0.0036, loss G: 7.0136\n",
      "Epoch [50/100] Batch 40/63             Loss D: 0.0027, loss G: 8.7724\n",
      "Epoch [50/100] Batch 50/63             Loss D: 0.0031, loss G: 7.4609\n",
      "Epoch [50/100] Batch 60/63             Loss D: 0.0040, loss G: 6.3869\n",
      "Epoch [51/100] Batch 0/63             Loss D: 0.0031, loss G: 6.8316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100] Batch 10/63             Loss D: 0.0031, loss G: 7.3963\n",
      "Epoch [51/100] Batch 20/63             Loss D: 0.0057, loss G: 6.5365\n",
      "Epoch [51/100] Batch 30/63             Loss D: 0.0041, loss G: 6.8767\n",
      "Epoch [51/100] Batch 40/63             Loss D: 0.0039, loss G: 7.4569\n",
      "Epoch [51/100] Batch 50/63             Loss D: 0.0039, loss G: 6.5641\n",
      "Epoch [51/100] Batch 60/63             Loss D: 0.0032, loss G: 7.6378\n",
      "Epoch [52/100] Batch 0/63             Loss D: 0.0032, loss G: 7.1631\n",
      "Epoch [52/100] Batch 10/63             Loss D: 0.0040, loss G: 6.7269\n",
      "Epoch [52/100] Batch 20/63             Loss D: 0.0050, loss G: 6.6530\n",
      "Epoch [52/100] Batch 30/63             Loss D: 0.0037, loss G: 6.6889\n",
      "Epoch [52/100] Batch 40/63             Loss D: 0.0103, loss G: 6.8200\n",
      "Epoch [52/100] Batch 50/63             Loss D: 0.0037, loss G: 6.9693\n",
      "Epoch [52/100] Batch 60/63             Loss D: 0.0032, loss G: 6.9836\n",
      "Epoch [53/100] Batch 0/63             Loss D: 0.0031, loss G: 7.2928\n",
      "Epoch [53/100] Batch 10/63             Loss D: 0.0032, loss G: 7.6909\n",
      "Epoch [53/100] Batch 20/63             Loss D: 0.0059, loss G: 6.8252\n",
      "Epoch [53/100] Batch 30/63             Loss D: 0.0065, loss G: 6.8056\n",
      "Epoch [53/100] Batch 40/63             Loss D: 0.0033, loss G: 7.2332\n",
      "Epoch [53/100] Batch 50/63             Loss D: 0.0028, loss G: 7.9999\n",
      "Epoch [53/100] Batch 60/63             Loss D: 0.0033, loss G: 6.6957\n",
      "Epoch [54/100] Batch 0/63             Loss D: 0.0033, loss G: 6.5317\n",
      "Epoch [54/100] Batch 10/63             Loss D: 0.0031, loss G: 7.0943\n",
      "Epoch [54/100] Batch 20/63             Loss D: 0.0035, loss G: 6.9839\n",
      "Epoch [54/100] Batch 30/63             Loss D: 0.0034, loss G: 6.6843\n",
      "Epoch [54/100] Batch 40/63             Loss D: 0.0032, loss G: 6.6426\n",
      "Epoch [54/100] Batch 50/63             Loss D: 0.0032, loss G: 6.6718\n",
      "Epoch [54/100] Batch 60/63             Loss D: 0.0037, loss G: 6.3644\n",
      "Epoch [55/100] Batch 0/63             Loss D: 0.0038, loss G: 6.1402\n",
      "Epoch [55/100] Batch 10/63             Loss D: 0.0055, loss G: 6.1684\n",
      "Epoch [55/100] Batch 20/63             Loss D: 0.0047, loss G: 6.4022\n",
      "Epoch [55/100] Batch 30/63             Loss D: 0.0046, loss G: 6.7193\n",
      "Epoch [55/100] Batch 40/63             Loss D: 0.0041, loss G: 6.8046\n",
      "Epoch [55/100] Batch 50/63             Loss D: 0.0053, loss G: 6.7728\n",
      "Epoch [55/100] Batch 60/63             Loss D: 0.0042, loss G: 6.5260\n",
      "Epoch [56/100] Batch 0/63             Loss D: 0.0215, loss G: 7.0689\n",
      "Epoch [56/100] Batch 10/63             Loss D: 0.0043, loss G: 6.5405\n",
      "Epoch [56/100] Batch 20/63             Loss D: 0.0041, loss G: 6.6200\n",
      "Epoch [56/100] Batch 30/63             Loss D: 0.0045, loss G: 6.7890\n",
      "Epoch [56/100] Batch 40/63             Loss D: 0.0035, loss G: 6.6514\n",
      "Epoch [56/100] Batch 50/63             Loss D: 0.0044, loss G: 6.8871\n",
      "Epoch [56/100] Batch 60/63             Loss D: 0.0050, loss G: 6.8013\n",
      "Epoch [57/100] Batch 0/63             Loss D: 0.0041, loss G: 6.8029\n",
      "Epoch [57/100] Batch 10/63             Loss D: 0.0036, loss G: 7.4500\n",
      "Epoch [57/100] Batch 20/63             Loss D: 0.0044, loss G: 7.1925\n",
      "Epoch [57/100] Batch 30/63             Loss D: 0.0037, loss G: 7.0596\n",
      "Epoch [57/100] Batch 40/63             Loss D: 0.0033, loss G: 7.7745\n",
      "Epoch [57/100] Batch 50/63             Loss D: 0.0029, loss G: 7.7194\n",
      "Epoch [57/100] Batch 60/63             Loss D: 0.0038, loss G: 7.1351\n",
      "Epoch [58/100] Batch 0/63             Loss D: 0.0032, loss G: 7.0778\n",
      "Epoch [58/100] Batch 10/63             Loss D: 0.0030, loss G: 7.6999\n",
      "Epoch [58/100] Batch 20/63             Loss D: 0.0035, loss G: 7.1710\n",
      "Epoch [58/100] Batch 30/63             Loss D: 0.0033, loss G: 6.8305\n",
      "Epoch [58/100] Batch 40/63             Loss D: 0.0038, loss G: 6.6931\n",
      "Epoch [58/100] Batch 50/63             Loss D: 0.0037, loss G: 6.6373\n",
      "Epoch [58/100] Batch 60/63             Loss D: 0.0027, loss G: 7.5993\n",
      "Epoch [59/100] Batch 0/63             Loss D: 0.0031, loss G: 6.9395\n",
      "Epoch [59/100] Batch 10/63             Loss D: 0.0031, loss G: 6.8765\n",
      "Epoch [59/100] Batch 20/63             Loss D: 0.0040, loss G: 7.5509\n",
      "Epoch [59/100] Batch 30/63             Loss D: 0.0032, loss G: 7.2793\n",
      "Epoch [59/100] Batch 40/63             Loss D: 0.0048, loss G: 6.7548\n",
      "Epoch [59/100] Batch 50/63             Loss D: 0.0040, loss G: 6.9757\n",
      "Epoch [59/100] Batch 60/63             Loss D: 0.0040, loss G: 7.2624\n",
      "Epoch [60/100] Batch 0/63             Loss D: 0.0034, loss G: 7.1487\n",
      "Epoch [60/100] Batch 10/63             Loss D: 0.0039, loss G: 7.9666\n",
      "Epoch [60/100] Batch 20/63             Loss D: 0.0092, loss G: 7.1822\n",
      "Epoch [60/100] Batch 30/63             Loss D: 0.0059, loss G: 7.0122\n",
      "Epoch [60/100] Batch 40/63             Loss D: 0.0044, loss G: 7.6676\n",
      "Epoch [60/100] Batch 50/63             Loss D: 0.0038, loss G: 6.7913\n",
      "Epoch [60/100] Batch 60/63             Loss D: 0.0033, loss G: 6.9239\n",
      "Epoch [61/100] Batch 0/63             Loss D: 0.0035, loss G: 6.7914\n",
      "Epoch [61/100] Batch 10/63             Loss D: 0.0031, loss G: 7.0675\n",
      "Epoch [61/100] Batch 20/63             Loss D: 0.0044, loss G: 6.8123\n",
      "Epoch [61/100] Batch 30/63             Loss D: 0.0060, loss G: 7.5811\n",
      "Epoch [61/100] Batch 40/63             Loss D: 0.0034, loss G: 7.2026\n",
      "Epoch [61/100] Batch 50/63             Loss D: 0.0028, loss G: 7.6287\n",
      "Epoch [61/100] Batch 60/63             Loss D: 0.0034, loss G: 7.0821\n",
      "Epoch [62/100] Batch 0/63             Loss D: 0.0034, loss G: 7.1527\n",
      "Epoch [62/100] Batch 10/63             Loss D: 0.0051, loss G: 7.9410\n",
      "Epoch [62/100] Batch 20/63             Loss D: 0.0039, loss G: 7.6198\n",
      "Epoch [62/100] Batch 30/63             Loss D: 0.0027, loss G: 8.1054\n",
      "Epoch [62/100] Batch 40/63             Loss D: 0.0038, loss G: 7.2451\n",
      "Epoch [62/100] Batch 50/63             Loss D: 0.0040, loss G: 7.2493\n",
      "Epoch [62/100] Batch 60/63             Loss D: 0.0030, loss G: 7.2962\n",
      "Epoch [63/100] Batch 0/63             Loss D: 0.0032, loss G: 7.2797\n",
      "Epoch [63/100] Batch 10/63             Loss D: 0.0035, loss G: 7.3912\n",
      "Epoch [63/100] Batch 20/63             Loss D: 0.0027, loss G: 7.4809\n",
      "Epoch [63/100] Batch 30/63             Loss D: 0.0026, loss G: 7.5162\n",
      "Epoch [63/100] Batch 40/63             Loss D: 0.0026, loss G: 7.5334\n",
      "Epoch [63/100] Batch 50/63             Loss D: 0.0027, loss G: 7.4617\n",
      "Epoch [63/100] Batch 60/63             Loss D: 0.0027, loss G: 7.4168\n",
      "Epoch [64/100] Batch 0/63             Loss D: 0.0027, loss G: 7.6241\n",
      "Epoch [64/100] Batch 10/63             Loss D: 0.0026, loss G: 7.4265\n",
      "Epoch [64/100] Batch 20/63             Loss D: 0.0030, loss G: 8.2531\n",
      "Epoch [64/100] Batch 30/63             Loss D: 0.0032, loss G: 7.2935\n",
      "Epoch [64/100] Batch 40/63             Loss D: 0.0032, loss G: 7.2188\n",
      "Epoch [64/100] Batch 50/63             Loss D: 0.0031, loss G: 7.3127\n",
      "Epoch [64/100] Batch 60/63             Loss D: 0.0031, loss G: 7.2896\n",
      "Epoch [65/100] Batch 0/63             Loss D: 0.0030, loss G: 7.1736\n",
      "Epoch [65/100] Batch 10/63             Loss D: 0.0027, loss G: 7.7775\n",
      "Epoch [65/100] Batch 20/63             Loss D: 0.0034, loss G: 6.6982\n",
      "Epoch [65/100] Batch 30/63             Loss D: 0.0033, loss G: 7.2257\n",
      "Epoch [65/100] Batch 40/63             Loss D: 0.0033, loss G: 7.1485\n",
      "Epoch [65/100] Batch 50/63             Loss D: 0.0029, loss G: 7.3060\n",
      "Epoch [65/100] Batch 60/63             Loss D: 0.0037, loss G: 7.4065\n",
      "Epoch [66/100] Batch 0/63             Loss D: 0.0027, loss G: 7.7268\n",
      "Epoch [66/100] Batch 10/63             Loss D: 0.0027, loss G: 7.7925\n",
      "Epoch [66/100] Batch 20/63             Loss D: 0.0030, loss G: 8.2104\n",
      "Epoch [66/100] Batch 30/63             Loss D: 0.0030, loss G: 7.1597\n",
      "Epoch [66/100] Batch 40/63             Loss D: 0.0030, loss G: 7.4066\n",
      "Epoch [66/100] Batch 50/63             Loss D: 0.0030, loss G: 7.2732\n",
      "Epoch [66/100] Batch 60/63             Loss D: 0.0031, loss G: 7.1686\n",
      "Epoch [67/100] Batch 0/63             Loss D: 0.0028, loss G: 7.1696\n",
      "Epoch [67/100] Batch 10/63             Loss D: 0.0031, loss G: 7.2130\n",
      "Epoch [67/100] Batch 20/63             Loss D: 0.0030, loss G: 7.8664\n",
      "Epoch [67/100] Batch 30/63             Loss D: 0.0030, loss G: 7.3848\n",
      "Epoch [67/100] Batch 40/63             Loss D: 0.0030, loss G: 7.5105\n",
      "Epoch [67/100] Batch 50/63             Loss D: 0.0032, loss G: 8.2290\n",
      "Epoch [67/100] Batch 60/63             Loss D: 0.0028, loss G: 7.5793\n",
      "Epoch [68/100] Batch 0/63             Loss D: 0.0027, loss G: 7.7066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100] Batch 10/63             Loss D: 0.0028, loss G: 7.5025\n",
      "Epoch [68/100] Batch 20/63             Loss D: 0.0032, loss G: 7.4868\n",
      "Epoch [68/100] Batch 30/63             Loss D: 0.0027, loss G: 7.8628\n",
      "Epoch [68/100] Batch 40/63             Loss D: 0.0027, loss G: 8.2699\n",
      "Epoch [68/100] Batch 50/63             Loss D: 0.0026, loss G: 7.6126\n",
      "Epoch [68/100] Batch 60/63             Loss D: 0.0024, loss G: 8.4780\n",
      "Epoch [69/100] Batch 0/63             Loss D: 0.0026, loss G: 7.7973\n",
      "Epoch [69/100] Batch 10/63             Loss D: 0.0028, loss G: 8.0577\n",
      "Epoch [69/100] Batch 20/63             Loss D: 0.0030, loss G: 8.0086\n",
      "Epoch [69/100] Batch 30/63             Loss D: 0.0033, loss G: 7.1297\n",
      "Epoch [69/100] Batch 40/63             Loss D: 0.0029, loss G: 8.1004\n",
      "Epoch [69/100] Batch 50/63             Loss D: 0.0026, loss G: 7.8059\n",
      "Epoch [69/100] Batch 60/63             Loss D: 0.0035, loss G: 8.3556\n",
      "Epoch [70/100] Batch 0/63             Loss D: 0.0026, loss G: 7.7838\n",
      "Epoch [70/100] Batch 10/63             Loss D: 0.0024, loss G: 7.6622\n",
      "Epoch [70/100] Batch 20/63             Loss D: 0.0024, loss G: 8.6118\n",
      "Epoch [70/100] Batch 30/63             Loss D: 0.0026, loss G: 7.8616\n",
      "Epoch [70/100] Batch 40/63             Loss D: 0.0026, loss G: 7.9422\n",
      "Epoch [70/100] Batch 50/63             Loss D: 0.0027, loss G: 7.5955\n",
      "Epoch [70/100] Batch 60/63             Loss D: 0.0027, loss G: 7.7955\n",
      "Epoch [71/100] Batch 0/63             Loss D: 0.0027, loss G: 7.9067\n",
      "Epoch [71/100] Batch 10/63             Loss D: 0.0026, loss G: 8.0129\n",
      "Epoch [71/100] Batch 20/63             Loss D: 0.0025, loss G: 7.9015\n",
      "Epoch [71/100] Batch 30/63             Loss D: 0.0030, loss G: 8.0962\n",
      "Epoch [71/100] Batch 40/63             Loss D: 0.0026, loss G: 7.9030\n",
      "Epoch [71/100] Batch 50/63             Loss D: 0.0024, loss G: 7.7662\n",
      "Epoch [71/100] Batch 60/63             Loss D: 0.0030, loss G: 8.2411\n",
      "Epoch [72/100] Batch 0/63             Loss D: 0.0026, loss G: 8.4697\n",
      "Epoch [72/100] Batch 10/63             Loss D: 0.0024, loss G: 8.1117\n",
      "Epoch [72/100] Batch 20/63             Loss D: 0.0029, loss G: 7.8393\n",
      "Epoch [72/100] Batch 30/63             Loss D: 0.0027, loss G: 7.9652\n",
      "Epoch [72/100] Batch 40/63             Loss D: 0.0024, loss G: 8.1930\n",
      "Epoch [72/100] Batch 50/63             Loss D: 0.0031, loss G: 7.6496\n",
      "Epoch [72/100] Batch 60/63             Loss D: 0.0032, loss G: 7.4284\n",
      "Epoch [73/100] Batch 0/63             Loss D: 0.0030, loss G: 7.6272\n",
      "Epoch [73/100] Batch 10/63             Loss D: 0.0030, loss G: 7.6174\n",
      "Epoch [73/100] Batch 20/63             Loss D: 0.0028, loss G: 8.1824\n",
      "Epoch [73/100] Batch 30/63             Loss D: 0.0032, loss G: 8.0421\n",
      "Epoch [73/100] Batch 40/63             Loss D: 0.0027, loss G: 8.0454\n",
      "Epoch [73/100] Batch 50/63             Loss D: 0.0025, loss G: 8.9552\n",
      "Epoch [73/100] Batch 60/63             Loss D: 0.0033, loss G: 7.6681\n",
      "Epoch [74/100] Batch 0/63             Loss D: 0.0029, loss G: 8.1796\n",
      "Epoch [74/100] Batch 10/63             Loss D: 0.0027, loss G: 9.2477\n",
      "Epoch [74/100] Batch 20/63             Loss D: 0.0060, loss G: 8.3843\n",
      "Epoch [74/100] Batch 30/63             Loss D: 0.0026, loss G: 8.5005\n",
      "Epoch [74/100] Batch 40/63             Loss D: 0.0037, loss G: 8.0100\n",
      "Epoch [74/100] Batch 50/63             Loss D: 0.0024, loss G: 9.7575\n",
      "Epoch [74/100] Batch 60/63             Loss D: 0.0024, loss G: 9.4349\n",
      "Epoch [75/100] Batch 0/63             Loss D: 0.0026, loss G: 9.0235\n",
      "Epoch [75/100] Batch 10/63             Loss D: 0.0027, loss G: 8.8096\n",
      "Epoch [75/100] Batch 20/63             Loss D: 0.0029, loss G: 8.8457\n",
      "Epoch [75/100] Batch 30/63             Loss D: 0.0026, loss G: 8.8975\n",
      "Epoch [75/100] Batch 40/63             Loss D: 0.0025, loss G: 9.6258\n",
      "Epoch [75/100] Batch 50/63             Loss D: 0.0027, loss G: 9.2848\n",
      "Epoch [75/100] Batch 60/63             Loss D: 0.0024, loss G: 9.1740\n",
      "Epoch [76/100] Batch 0/63             Loss D: 0.0026, loss G: 8.8527\n",
      "Epoch [76/100] Batch 10/63             Loss D: 0.0024, loss G: 8.7795\n",
      "Epoch [76/100] Batch 20/63             Loss D: 0.0031, loss G: 9.1204\n",
      "Epoch [76/100] Batch 30/63             Loss D: 0.0031, loss G: 8.5306\n",
      "Epoch [76/100] Batch 40/63             Loss D: 0.0029, loss G: 8.8236\n",
      "Epoch [76/100] Batch 50/63             Loss D: 0.0028, loss G: 8.8803\n",
      "Epoch [76/100] Batch 60/63             Loss D: 0.0038, loss G: 9.3250\n",
      "Epoch [77/100] Batch 0/63             Loss D: 0.0292, loss G: 9.1969\n",
      "Epoch [77/100] Batch 10/63             Loss D: 0.0048, loss G: 10.0563\n",
      "Epoch [77/100] Batch 20/63             Loss D: 0.0034, loss G: 9.8448\n",
      "Epoch [77/100] Batch 30/63             Loss D: 0.0028, loss G: 9.7775\n",
      "Epoch [77/100] Batch 40/63             Loss D: 0.0044, loss G: 10.4705\n",
      "Epoch [77/100] Batch 50/63             Loss D: 0.0032, loss G: 9.8321\n",
      "Epoch [77/100] Batch 60/63             Loss D: 0.0045, loss G: 8.9332\n",
      "Epoch [78/100] Batch 0/63             Loss D: 0.0040, loss G: 9.3948\n",
      "Epoch [78/100] Batch 10/63             Loss D: 0.0029, loss G: 9.3157\n",
      "Epoch [78/100] Batch 20/63             Loss D: 0.0030, loss G: 9.0545\n",
      "Epoch [78/100] Batch 30/63             Loss D: 0.0031, loss G: 8.6436\n",
      "Epoch [78/100] Batch 40/63             Loss D: 0.0030, loss G: 8.1228\n",
      "Epoch [78/100] Batch 50/63             Loss D: 0.0079, loss G: 8.5695\n",
      "Epoch [78/100] Batch 60/63             Loss D: 0.0031, loss G: 8.6896\n",
      "Epoch [79/100] Batch 0/63             Loss D: 0.0028, loss G: 8.4121\n",
      "Epoch [79/100] Batch 10/63             Loss D: 0.0025, loss G: 8.4133\n",
      "Epoch [79/100] Batch 20/63             Loss D: 0.0026, loss G: 8.2810\n",
      "Epoch [79/100] Batch 30/63             Loss D: 0.0036, loss G: 7.8545\n",
      "Epoch [79/100] Batch 40/63             Loss D: 0.0029, loss G: 8.3618\n",
      "Epoch [79/100] Batch 50/63             Loss D: 0.0031, loss G: 8.0284\n",
      "Epoch [79/100] Batch 60/63             Loss D: 0.0029, loss G: 8.1570\n",
      "Epoch [80/100] Batch 0/63             Loss D: 0.0027, loss G: 8.2141\n",
      "Epoch [80/100] Batch 10/63             Loss D: 0.0030, loss G: 8.4386\n",
      "Epoch [80/100] Batch 20/63             Loss D: 0.0034, loss G: 9.2558\n",
      "Epoch [80/100] Batch 30/63             Loss D: 0.0029, loss G: 8.7154\n",
      "Epoch [80/100] Batch 40/63             Loss D: 0.0062, loss G: 7.5511\n",
      "Epoch [80/100] Batch 50/63             Loss D: 0.0057, loss G: 7.0583\n",
      "Epoch [80/100] Batch 60/63             Loss D: 0.0045, loss G: 8.1064\n",
      "Epoch [81/100] Batch 0/63             Loss D: 0.0050, loss G: 8.2003\n",
      "Epoch [81/100] Batch 10/63             Loss D: 0.0043, loss G: 7.4912\n",
      "Epoch [81/100] Batch 20/63             Loss D: 0.0048, loss G: 6.8881\n",
      "Epoch [81/100] Batch 30/63             Loss D: 0.0039, loss G: 6.9834\n",
      "Epoch [81/100] Batch 40/63             Loss D: 0.0048, loss G: 7.2937\n",
      "Epoch [81/100] Batch 50/63             Loss D: 0.0043, loss G: 8.0843\n",
      "Epoch [81/100] Batch 60/63             Loss D: 0.0041, loss G: 8.4280\n",
      "Epoch [82/100] Batch 0/63             Loss D: 0.0046, loss G: 8.5061\n",
      "Epoch [82/100] Batch 10/63             Loss D: 0.0058, loss G: 8.2573\n",
      "Epoch [82/100] Batch 20/63             Loss D: 0.0042, loss G: 8.3296\n",
      "Epoch [82/100] Batch 30/63             Loss D: 0.0045, loss G: 8.2615\n",
      "Epoch [82/100] Batch 40/63             Loss D: 0.0111, loss G: 8.3327\n",
      "Epoch [82/100] Batch 50/63             Loss D: 0.0029, loss G: 8.7802\n",
      "Epoch [82/100] Batch 60/63             Loss D: 0.0037, loss G: 9.3021\n",
      "Epoch [83/100] Batch 0/63             Loss D: 0.0030, loss G: 9.4322\n",
      "Epoch [83/100] Batch 10/63             Loss D: 0.0026, loss G: 9.3858\n",
      "Epoch [83/100] Batch 20/63             Loss D: 0.0030, loss G: 8.5721\n",
      "Epoch [83/100] Batch 30/63             Loss D: 0.0028, loss G: 8.5653\n",
      "Epoch [83/100] Batch 40/63             Loss D: 0.0042, loss G: 8.2416\n",
      "Epoch [83/100] Batch 50/63             Loss D: 0.0031, loss G: 8.6987\n",
      "Epoch [83/100] Batch 60/63             Loss D: 0.0028, loss G: 8.6061\n",
      "Epoch [84/100] Batch 0/63             Loss D: 0.0033, loss G: 8.8242\n",
      "Epoch [84/100] Batch 10/63             Loss D: 0.0032, loss G: 8.7967\n",
      "Epoch [84/100] Batch 20/63             Loss D: 0.0027, loss G: 8.6786\n",
      "Epoch [84/100] Batch 30/63             Loss D: 0.0037, loss G: 7.3439\n",
      "Epoch [84/100] Batch 40/63             Loss D: 0.0038, loss G: 7.2056\n",
      "Epoch [84/100] Batch 50/63             Loss D: 0.0031, loss G: 7.5939\n",
      "Epoch [84/100] Batch 60/63             Loss D: 0.0036, loss G: 7.1801\n",
      "Epoch [85/100] Batch 0/63             Loss D: 0.0044, loss G: 6.9292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100] Batch 10/63             Loss D: 0.0049, loss G: 7.6048\n",
      "Epoch [85/100] Batch 20/63             Loss D: 0.0029, loss G: 9.2709\n",
      "Epoch [85/100] Batch 30/63             Loss D: 0.0031, loss G: 8.2215\n",
      "Epoch [85/100] Batch 40/63             Loss D: 0.0033, loss G: 8.2893\n",
      "Epoch [85/100] Batch 50/63             Loss D: 0.0035, loss G: 8.2790\n",
      "Epoch [85/100] Batch 60/63             Loss D: 0.0027, loss G: 8.4079\n",
      "Epoch [86/100] Batch 0/63             Loss D: 0.0029, loss G: 8.1943\n",
      "Epoch [86/100] Batch 10/63             Loss D: 0.0028, loss G: 7.9415\n",
      "Epoch [86/100] Batch 20/63             Loss D: 0.0039, loss G: 8.1567\n",
      "Epoch [86/100] Batch 30/63             Loss D: 0.0029, loss G: 8.4068\n",
      "Epoch [86/100] Batch 40/63             Loss D: 0.0041, loss G: 8.7428\n",
      "Epoch [86/100] Batch 50/63             Loss D: 0.0029, loss G: 8.7982\n",
      "Epoch [86/100] Batch 60/63             Loss D: 0.0034, loss G: 8.9137\n",
      "Epoch [87/100] Batch 0/63             Loss D: 0.0029, loss G: 8.3654\n",
      "Epoch [87/100] Batch 10/63             Loss D: 0.0024, loss G: 9.4757\n",
      "Epoch [87/100] Batch 20/63             Loss D: 0.0029, loss G: 10.0213\n",
      "Epoch [87/100] Batch 30/63             Loss D: 0.0033, loss G: 10.1322\n",
      "Epoch [87/100] Batch 40/63             Loss D: 0.0024, loss G: 9.4566\n",
      "Epoch [87/100] Batch 50/63             Loss D: 0.0026, loss G: 10.0281\n",
      "Epoch [87/100] Batch 60/63             Loss D: 0.0033, loss G: 8.4309\n",
      "Epoch [88/100] Batch 0/63             Loss D: 0.0032, loss G: 9.0483\n",
      "Epoch [88/100] Batch 10/63             Loss D: 0.0034, loss G: 8.0624\n",
      "Epoch [88/100] Batch 20/63             Loss D: 0.0034, loss G: 9.5201\n",
      "Epoch [88/100] Batch 30/63             Loss D: 0.0033, loss G: 8.1021\n",
      "Epoch [88/100] Batch 40/63             Loss D: 0.0030, loss G: 8.4091\n",
      "Epoch [88/100] Batch 50/63             Loss D: 0.0024, loss G: 8.9673\n",
      "Epoch [88/100] Batch 60/63             Loss D: 0.0026, loss G: 9.8143\n",
      "Epoch [89/100] Batch 0/63             Loss D: 0.0030, loss G: 8.7581\n",
      "Epoch [89/100] Batch 10/63             Loss D: 0.0026, loss G: 9.8211\n",
      "Epoch [89/100] Batch 20/63             Loss D: 0.0025, loss G: 8.7566\n",
      "Epoch [89/100] Batch 30/63             Loss D: 0.0025, loss G: 9.7921\n",
      "Epoch [89/100] Batch 40/63             Loss D: 0.0024, loss G: 9.9646\n",
      "Epoch [89/100] Batch 50/63             Loss D: 0.0025, loss G: 8.8431\n",
      "Epoch [89/100] Batch 60/63             Loss D: 0.0025, loss G: 8.8106\n",
      "Epoch [90/100] Batch 0/63             Loss D: 0.0023, loss G: 8.8118\n",
      "Epoch [90/100] Batch 10/63             Loss D: 0.0025, loss G: 8.8677\n",
      "Epoch [90/100] Batch 20/63             Loss D: 0.0027, loss G: 8.8957\n",
      "Epoch [90/100] Batch 30/63             Loss D: 0.0029, loss G: 8.8757\n",
      "Epoch [90/100] Batch 40/63             Loss D: 0.0028, loss G: 8.8933\n",
      "Epoch [90/100] Batch 50/63             Loss D: 0.0026, loss G: 8.7448\n",
      "Epoch [90/100] Batch 60/63             Loss D: 0.0023, loss G: 9.5003\n",
      "Epoch [91/100] Batch 0/63             Loss D: 0.0026, loss G: 8.6665\n",
      "Epoch [91/100] Batch 10/63             Loss D: 0.0026, loss G: 9.3723\n",
      "Epoch [91/100] Batch 20/63             Loss D: 0.0026, loss G: 8.4968\n",
      "Epoch [91/100] Batch 30/63             Loss D: 0.0030, loss G: 8.3511\n",
      "Epoch [91/100] Batch 40/63             Loss D: 0.0027, loss G: 8.4512\n",
      "Epoch [91/100] Batch 50/63             Loss D: 0.0026, loss G: 9.4310\n",
      "Epoch [91/100] Batch 60/63             Loss D: 0.0031, loss G: 8.8267\n",
      "Epoch [92/100] Batch 0/63             Loss D: 0.0030, loss G: 8.3151\n",
      "Epoch [92/100] Batch 10/63             Loss D: 0.0027, loss G: 9.1405\n",
      "Epoch [92/100] Batch 20/63             Loss D: 0.0036, loss G: 9.8139\n",
      "Epoch [92/100] Batch 30/63             Loss D: 0.0022, loss G: 9.8682\n",
      "Epoch [92/100] Batch 40/63             Loss D: 0.0024, loss G: 8.7524\n",
      "Epoch [92/100] Batch 50/63             Loss D: 0.0027, loss G: 8.3763\n",
      "Epoch [92/100] Batch 60/63             Loss D: 0.0051, loss G: 7.2769\n",
      "Epoch [93/100] Batch 0/63             Loss D: 0.0044, loss G: 7.8332\n",
      "Epoch [93/100] Batch 10/63             Loss D: 0.0027, loss G: 9.4535\n",
      "Epoch [93/100] Batch 20/63             Loss D: 0.0024, loss G: 8.8494\n",
      "Epoch [93/100] Batch 30/63             Loss D: 0.0038, loss G: 9.6678\n",
      "Epoch [93/100] Batch 40/63             Loss D: 0.0035, loss G: 8.9891\n",
      "Epoch [93/100] Batch 50/63             Loss D: 0.0028, loss G: 9.8950\n",
      "Epoch [93/100] Batch 60/63             Loss D: 0.0031, loss G: 8.3223\n",
      "Epoch [94/100] Batch 0/63             Loss D: 0.0080, loss G: 8.1850\n",
      "Epoch [94/100] Batch 10/63             Loss D: 0.0039, loss G: 8.2065\n",
      "Epoch [94/100] Batch 20/63             Loss D: 0.0026, loss G: 8.9642\n",
      "Epoch [94/100] Batch 30/63             Loss D: 0.0028, loss G: 8.3944\n",
      "Epoch [94/100] Batch 40/63             Loss D: 0.0027, loss G: 9.2464\n",
      "Epoch [94/100] Batch 50/63             Loss D: 0.0030, loss G: 8.2738\n",
      "Epoch [94/100] Batch 60/63             Loss D: 0.0036, loss G: 7.7399\n",
      "Epoch [95/100] Batch 0/63             Loss D: 0.0033, loss G: 8.6279\n",
      "Epoch [95/100] Batch 10/63             Loss D: 0.0028, loss G: 8.8172\n",
      "Epoch [95/100] Batch 20/63             Loss D: 0.0024, loss G: 9.7864\n",
      "Epoch [95/100] Batch 30/63             Loss D: 0.0024, loss G: 8.8946\n",
      "Epoch [95/100] Batch 40/63             Loss D: 0.0025, loss G: 8.8909\n",
      "Epoch [95/100] Batch 50/63             Loss D: 0.0027, loss G: 8.7484\n",
      "Epoch [95/100] Batch 60/63             Loss D: 0.0023, loss G: 8.7082\n",
      "Epoch [96/100] Batch 0/63             Loss D: 0.0024, loss G: 8.7784\n",
      "Epoch [96/100] Batch 10/63             Loss D: 0.0026, loss G: 8.5575\n",
      "Epoch [96/100] Batch 20/63             Loss D: 0.0025, loss G: 9.1159\n",
      "Epoch [96/100] Batch 30/63             Loss D: 0.0025, loss G: 8.3910\n",
      "Epoch [96/100] Batch 40/63             Loss D: 0.0030, loss G: 8.1225\n",
      "Epoch [96/100] Batch 50/63             Loss D: 0.0028, loss G: 7.8177\n",
      "Epoch [96/100] Batch 60/63             Loss D: 0.0029, loss G: 8.0486\n",
      "Epoch [97/100] Batch 0/63             Loss D: 0.0026, loss G: 9.4193\n",
      "Epoch [97/100] Batch 10/63             Loss D: 0.0036, loss G: 7.8747\n",
      "Epoch [97/100] Batch 20/63             Loss D: 0.0031, loss G: 8.3663\n",
      "Epoch [97/100] Batch 30/63             Loss D: 0.0028, loss G: 8.4249\n",
      "Epoch [97/100] Batch 40/63             Loss D: 0.0028, loss G: 10.1699\n",
      "Epoch [97/100] Batch 50/63             Loss D: 0.0028, loss G: 8.4920\n",
      "Epoch [97/100] Batch 60/63             Loss D: 0.0024, loss G: 10.1162\n",
      "Epoch [98/100] Batch 0/63             Loss D: 0.0029, loss G: 8.5832\n",
      "Epoch [98/100] Batch 10/63             Loss D: 0.0027, loss G: 8.3175\n",
      "Epoch [98/100] Batch 20/63             Loss D: 0.0025, loss G: 9.4199\n",
      "Epoch [98/100] Batch 30/63             Loss D: 0.0028, loss G: 8.4219\n",
      "Epoch [98/100] Batch 40/63             Loss D: 0.0025, loss G: 8.6159\n",
      "Epoch [98/100] Batch 50/63             Loss D: 0.0025, loss G: 8.4658\n",
      "Epoch [98/100] Batch 60/63             Loss D: 0.0028, loss G: 8.2401\n",
      "Epoch [99/100] Batch 0/63             Loss D: 0.0026, loss G: 8.3585\n",
      "Epoch [99/100] Batch 10/63             Loss D: 0.0054, loss G: 8.6928\n",
      "Epoch [99/100] Batch 20/63             Loss D: 0.0031, loss G: 7.9991\n",
      "Epoch [99/100] Batch 30/63             Loss D: 0.0050, loss G: 9.7406\n",
      "Epoch [99/100] Batch 40/63             Loss D: 0.0030, loss G: 9.4303\n",
      "Epoch [99/100] Batch 50/63             Loss D: 0.0061, loss G: 9.9823\n",
      "Epoch [99/100] Batch 60/63             Loss D: 0.0027, loss G: 8.8456\n"
     ]
    }
   ],
   "source": [
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "G_loss_list = []\n",
    "D_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, data_all in enumerate(train_loader):\n",
    "    \n",
    "        step = 0\n",
    "        data, y = data_all\n",
    "        data = data.to(device)\n",
    "        y = y.to(device)\n",
    "        real_data = data.to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "        noise = torch.randn(real_data.size(0), len(required_params), 64, device=device)\n",
    "\n",
    "        D_opt.zero_grad()\n",
    "        G_opt.zero_grad()\n",
    "\n",
    "        fake = generator(noise)\n",
    "#         Ssim_loss = criterion_ssim(real_data, noise)\n",
    "        # classificator\n",
    "        output_class = model_inception(data)\n",
    "        loss_class = criterion(output_class, y)\n",
    "\n",
    "        disc_real = discriminator(real_data)        \n",
    "        D_loss_1 = criterion_gan(disc_real, torch.ones_like(disc_real))        \n",
    "        disc_fake = discriminator(fake)\n",
    "        D_loss_2 = criterion_gan(disc_fake, torch.zeros_like(disc_fake))\n",
    "        D_loss = (D_loss_1 + D_loss_2) / 2 + loss_class * 0.001\n",
    "#         + abs(Ssim_loss)\n",
    "        D_opt.zero_grad()\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        D_opt.step()\n",
    "\n",
    "        output = discriminator(fake)\n",
    "        G_loss = criterion_gan(output, torch.ones_like(output))\n",
    "        G_opt.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_opt.step()\n",
    "\n",
    "        G_loss_list.append(G_loss.item())\n",
    "        D_loss_list.append(D_loss.item())\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {idx}/{len(train_loader)} \\\n",
    "            Loss D: {D_loss:.4f}, loss G: {G_loss:.4f}\")\n",
    "\n",
    "            step += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_loss_list,label=\"G\")\n",
    "plt.plot(D_loss_list,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('/home/sharifullina/thesis/losses/loss_cGAN_time_series_inception_class.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_rate 1\n",
      "L 64\n",
      "window 64\n",
      "step 4\n"
     ]
    }
   ],
   "source": [
    "scales_vectors = np.array([torch_dataset.feature_scales[f] for f in range(len(required_params))])\n",
    "\n",
    "sparse_rate = torch_dataset.sparse_rate\n",
    "print('sparse_rate', sparse_rate)\n",
    "L = 64\n",
    "print('L', L)\n",
    "# window = L * sparse_rate\n",
    "window = 64\n",
    "print('window', window)\n",
    "step = 4\n",
    "print('step', step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well_17.json'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_well = wells[22]\n",
    "test_well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(logs[22]['data'], columns=[x['name'] for x in logs[22]['curves']])\n",
    "X_test = X_test[required_params]\n",
    "X_test = X_test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name = 'DEPTH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>ROPA</th>\n",
       "      <th>HKLA</th>\n",
       "      <th>WOB</th>\n",
       "      <th>SPPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>96.39</td>\n",
       "      <td>0.19</td>\n",
       "      <td>84.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224.0</td>\n",
       "      <td>3.84</td>\n",
       "      <td>97.11</td>\n",
       "      <td>0.60</td>\n",
       "      <td>77.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>225.0</td>\n",
       "      <td>3.51</td>\n",
       "      <td>96.76</td>\n",
       "      <td>0.58</td>\n",
       "      <td>85.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>226.0</td>\n",
       "      <td>21.85</td>\n",
       "      <td>97.11</td>\n",
       "      <td>0.36</td>\n",
       "      <td>53.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>227.0</td>\n",
       "      <td>33.79</td>\n",
       "      <td>94.76</td>\n",
       "      <td>2.55</td>\n",
       "      <td>53.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405</th>\n",
       "      <td>3628.0</td>\n",
       "      <td>15.04</td>\n",
       "      <td>156.24</td>\n",
       "      <td>5.77</td>\n",
       "      <td>218.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>3629.0</td>\n",
       "      <td>15.04</td>\n",
       "      <td>155.61</td>\n",
       "      <td>6.36</td>\n",
       "      <td>218.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3407</th>\n",
       "      <td>3630.0</td>\n",
       "      <td>15.04</td>\n",
       "      <td>155.27</td>\n",
       "      <td>6.58</td>\n",
       "      <td>218.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>3631.0</td>\n",
       "      <td>15.04</td>\n",
       "      <td>155.71</td>\n",
       "      <td>6.26</td>\n",
       "      <td>218.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409</th>\n",
       "      <td>3632.0</td>\n",
       "      <td>15.01</td>\n",
       "      <td>155.40</td>\n",
       "      <td>6.62</td>\n",
       "      <td>218.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3410 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       DEPTH   ROPA    HKLA   WOB    SPPA\n",
       "0      223.0   1.50   96.39  0.19   84.75\n",
       "1      224.0   3.84   97.11  0.60   77.10\n",
       "2      225.0   3.51   96.76  0.58   85.56\n",
       "3      226.0  21.85   97.11  0.36   53.67\n",
       "4      227.0  33.79   94.76  2.55   53.96\n",
       "...      ...    ...     ...   ...     ...\n",
       "3405  3628.0  15.04  156.24  5.77  218.70\n",
       "3406  3629.0  15.04  155.61  6.36  218.79\n",
       "3407  3630.0  15.04  155.27  6.58  218.60\n",
       "3408  3631.0  15.04  155.71  6.26  218.71\n",
       "3409  3632.0  15.01  155.40  6.62  218.65\n",
       "\n",
       "[3410 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_window_transform(x, s, latent_noise_rate=0):\n",
    "    x = x[np.arange(0, len(x), sparse_rate), :]\n",
    "    mean_x = x.mean(axis=0)\n",
    "    x = (x - mean_x)/s\n",
    "    with torch.no_grad():\n",
    "        x = torch.Tensor(x).view(1, x.shape[0], -1).to(device)\n",
    "        z = generator(x.transpose(1, 2))\n",
    "    \n",
    "    return z.transpose(1, 2)[0].cpu().data.numpy()*s + mean_x, z.transpose(1, 2)[0].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rolling_transform(X, w, scales, step=12):\n",
    "    Z = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(X) - w, step)):\n",
    "        Z.append(apply_window_transform(X[i: i+w], scales))\n",
    "        \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|     | 807/837 [00:04<00:00, 205.49it/s]"
     ]
    }
   ],
   "source": [
    "Z = apply_rolling_transform(X_test[required_params].values, window, scales_vectors, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = []\n",
    "generated = []\n",
    "\n",
    "t = X_test[y_name].astype(float).values\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, curve_name in enumerate(required_params):\n",
    "    plt.subplot(1, len(df.columns) - 1, i + 1)\n",
    "#     for zi, x in tqdm(enumerate(range(0, len(df_well) - window, step))):\n",
    "#         plt.plot(Z[zi][0][:, i], t[np.arange(x, x+window, sparse_rate)], color='C1', alpha=0.5, lw=0.5)\n",
    "\n",
    "    zis = range(0, len(X_test) - window, step)\n",
    "    A = np.zeros((len(zis), len(X_test))) * np.nan\n",
    "    for zi, x in tqdm(enumerate(zis)):\n",
    "        A[zi, np.arange(x, x+window, sparse_rate)] = Z[zi][0][:, i]\n",
    "    \n",
    "    A = np.nanmean(A, axis=0)\n",
    "    not_nan_inds = np.where(~np.isnan(A))[0]\n",
    "    \n",
    "    original.append(X_test[curve_name].astype(float).values)\n",
    "    generated.append(A[not_nan_inds])\n",
    "    \n",
    "    plt.plot(A[not_nan_inds], t[not_nan_inds], color='C1', lw=1)\n",
    "    plt.plot(X_test[curve_name].astype(float).values, t, lw=1, color='C0', alpha=0.5)\n",
    "    plt.grid()\n",
    "    axis = plt.gca()\n",
    "    axis.invert_yaxis()\n",
    "    axis.xaxis.tick_top()\n",
    "    axis.xaxis.set_label_position('top')\n",
    "    if i > 0: axis.set_yticklabels([])\n",
    "\n",
    "    if i == 0:\n",
    "        plt.ylabel(f'{y_name}')\n",
    "    plt.xlabel(f'{curve_name}')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(original).T\n",
    "df1.columns = required_params\n",
    "df1 = df1.assign(WELL=lambda x: 'original')\n",
    "\n",
    "df2 = pd.DataFrame(generated).T\n",
    "df2.columns = required_params\n",
    "df2 = df2.assign(WELL=lambda x: 'generated')\n",
    "\n",
    "df_some = pd.concat([df1, df2], ignore_index = True)\n",
    "df_some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in required_params[:-1]:\n",
    "    sns.displot(data=df_some, \n",
    "            x=i, hue='WELL', kind='kde',\n",
    "            fill=True, palette=sns.color_palette('bright')[:2], height=5, aspect=1.5\n",
    "               )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8fvVJ3YbdTY7"
   ],
   "name": "cGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
